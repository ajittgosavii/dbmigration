import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from datetime import datetime, timedelta
import time
import re
import json
import asyncio
import logging
from typing import Dict, List, Tuple, Optional, Set, Union
from dataclasses import dataclass, field
from enum import Enum
import hashlib
import uuid
import sqlite3
from pathlib import Path
import difflib
import ast

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Page configuration
st.set_page_config(
    page_title="Enterprise Database Migration Analyzer",
    page_icon="üóÑÔ∏è",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Enhanced Professional CSS styling
st.markdown("""
<style>
    .main-header {
        background: linear-gradient(135deg, #1e3a8a 0%, #1e40af 50%, #1d4ed8 100%);
        padding: 2rem;
        border-radius: 12px;
        color: white;
        text-align: center;
        margin-bottom: 2rem;
        box-shadow: 0 4px 20px rgba(30,58,138,0.3);
        border: 1px solid rgba(255,255,255,0.1);
    }
    
    .main-header h1 {
        margin: 0 0 0.5rem 0;
        font-size: 2.5rem;
        font-weight: 700;
        text-shadow: 0 2px 4px rgba(0,0,0,0.3);
    }
    
    .enterprise-badge {
        background: linear-gradient(45deg, #10b981, #059669);
        padding: 0.5rem 1rem;
        border-radius: 20px;
        color: white;
        font-weight: bold;
        display: inline-block;
        margin-top: 0.5rem;
        box-shadow: 0 2px 8px rgba(16,185,129,0.3);
    }
    
    .analysis-card {
        background: linear-gradient(135deg, #ffffff 0%, #f8fafc 100%);
        padding: 1.5rem;
        border-radius: 10px;
        margin: 1rem 0;
        box-shadow: 0 4px 12px rgba(0,0,0,0.1);
        border-left: 4px solid #3b82f6;
        border: 1px solid #e5e7eb;
        transition: transform 0.2s ease;
    }
    
    .analysis-card:hover {
        transform: translateY(-2px);
        box-shadow: 0 6px 20px rgba(0,0,0,0.15);
    }
    
    .enterprise-card {
        background: linear-gradient(135deg, #f0f9ff 0%, #e0f2fe 100%);
        padding: 1.5rem;
        border-radius: 10px;
        margin: 1rem 0;
        box-shadow: 0 4px 12px rgba(0,0,0,0.1);
        border-left: 4px solid #0ea5e9;
        border: 1px solid #e5e7eb;
    }
    
    .cost-card {
        background: linear-gradient(135deg, #ecfdf5 0%, #d1fae5 100%);
        padding: 1.5rem;
        border-radius: 10px;
        margin: 1rem 0;
        box-shadow: 0 4px 12px rgba(0,0,0,0.1);
        border-left: 4px solid #10b981;
        border: 1px solid #e5e7eb;
    }
    
    .security-card {
        background: linear-gradient(135deg, #fef3c7 0%, #fde68a 100%);
        padding: 1.5rem;
        border-radius: 10px;
        margin: 1rem 0;
        box-shadow: 0 4px 12px rgba(0,0,0,0.1);
        border-left: 4px solid #f59e0b;
        border: 1px solid #e5e7eb;
    }
    
    .collaboration-card {
        background: linear-gradient(135deg, #faf5ff 0%, #f3e8ff 100%);
        padding: 1.5rem;
        border-radius: 10px;
        margin: 1rem 0;
        box-shadow: 0 4px 12px rgba(0,0,0,0.1);
        border-left: 4px solid #a855f7;
        border: 1px solid #e5e7eb;
    }
    
    .ai-enhanced-card {
        background: linear-gradient(135deg, #fef2f2 0%, #fee2e2 100%);
        padding: 1.5rem;
        border-radius: 10px;
        margin: 1rem 0;
        box-shadow: 0 4px 12px rgba(0,0,0,0.1);
        border-left: 4px solid #ef4444;
        border: 1px solid #e5e7eb;
        position: relative;
    }
    
    .ai-enhanced-card::before {
        content: "ü§ñ AI Enhanced";
        position: absolute;
        top: -10px;
        right: 10px;
        background: #ef4444;
        color: white;
        padding: 0.25rem 0.5rem;
        border-radius: 4px;
        font-size: 0.75rem;
        font-weight: bold;
    }
    
    .metric-card {
        background: #ffffff;
        padding: 1.5rem;
        border-radius: 10px;
        border-left: 4px solid #3b82f6;
        margin: 1rem 0;
        box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        border: 1px solid #e5e7eb;
        transition: all 0.2s ease;
    }
    
    .metric-card:hover {
        transform: translateY(-1px);
        box-shadow: 0 4px 16px rgba(0,0,0,0.15);
    }
    
    .progress-indicator {
        width: 100%;
        height: 8px;
        background: #e5e7eb;
        border-radius: 4px;
        overflow: hidden;
        margin: 0.5rem 0;
    }
    
    .progress-fill {
        height: 100%;
        border-radius: 4px;
        transition: width 0.3s ease;
    }
    
    .progress-high { background: linear-gradient(90deg, #ef4444, #dc2626); }
    .progress-medium { background: linear-gradient(90deg, #f59e0b, #d97706); }
    .progress-low { background: linear-gradient(90deg, #22c55e, #16a34a); }
    
    .feature-badge {
        display: inline-block;
        padding: 0.25rem 0.5rem;
        margin: 0.25rem;
        border-radius: 12px;
        font-size: 0.75rem;
        font-weight: bold;
        color: white;
    }
    
    .badge-new { background: #ef4444; }
    .badge-enhanced { background: #f59e0b; }
    .badge-ai { background: #8b5cf6; }
    .badge-enterprise { background: #059669; }
    
    .sidebar-info {
        background: linear-gradient(135deg, #f8fafc 0%, #f1f5f9 100%);
        padding: 1rem;
        border-radius: 8px;
        margin: 0.5rem 0;
        border-left: 3px solid #3b82f6;
        font-size: 0.9rem;
    }
    
    .success-banner {
        background: linear-gradient(135deg, #dcfce7 0%, #bbf7d0 100%);
        border: 1px solid #22c55e;
        border-radius: 8px;
        padding: 1rem;
        margin: 1rem 0;
        color: #166534;
    }
    
    .warning-banner {
        background: linear-gradient(135deg, #fef3c7 0%, #fde68a 100%);
        border: 1px solid #f59e0b;
        border-radius: 8px;
        padding: 1rem;
        margin: 1rem 0;
        color: #92400e;
    }
    
    .error-banner {
        background: linear-gradient(135deg, #fee2e2 0%, #fecaca 100%);
        border: 1px solid #ef4444;
        border-radius: 8px;
        padding: 1rem;
        margin: 1rem 0;
        color: #991b1b;
    }
    
    .cost-optimization {
        background: linear-gradient(135deg, #f0fdf4 0%, #dcfce7 100%);
        border: 2px solid #22c55e;
        border-radius: 12px;
        padding: 1.5rem;
        margin: 1rem 0;
    }
    
    .risk-matrix {
        display: grid;
        grid-template-columns: repeat(3, 1fr);
        gap: 1rem;
        margin: 1rem 0;
    }
    
    .risk-cell {
        padding: 1rem;
        border-radius: 8px;
        text-align: center;
        font-weight: bold;
        color: white;
    }
    
    .risk-high { background: #ef4444; }
    .risk-medium { background: #f59e0b; }
    .risk-low { background: #22c55e; }
    
    .collaboration-status {
        display: flex;
        align-items: center;
        gap: 0.5rem;
        padding: 0.5rem;
        background: #f8fafc;
        border-radius: 6px;
        margin: 0.25rem 0;
    }
    
    .status-online { color: #22c55e; }
    .status-away { color: #f59e0b; }
    .status-offline { color: #6b7280; }
</style>
""", unsafe_allow_html=True)

# Enhanced Enums and Data Classes
class DatabaseEngine(Enum):
    """Supported database engines"""
    MYSQL = "mysql"
    POSTGRESQL = "postgresql"
    ORACLE = "oracle"
    SQL_SERVER = "sql_server"
    MONGODB = "mongodb"
    REDIS = "redis"
    CASSANDRA = "cassandra"

class ComplexityLevel(Enum):
    """Migration complexity levels"""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    VERY_HIGH = "very_high"

class CompatibilityStatus(Enum):
    """Compatibility status for objects"""
    COMPATIBLE = "compatible"
    PARTIALLY_COMPATIBLE = "partially_compatible"
    INCOMPATIBLE = "incompatible"
    REQUIRES_MANUAL_REVIEW = "requires_manual_review"

class AnalysisType(Enum):
    """Types of AI analysis"""
    MIGRATION_STRATEGY = "migration_strategy"
    RISK_ASSESSMENT = "risk_assessment"
    PERFORMANCE_PREDICTION = "performance_prediction"
    COST_OPTIMIZATION = "cost_optimization"
    TIMELINE_ESTIMATION = "timeline_estimation"
    TESTING_STRATEGY = "testing_strategy"
    SECURITY_ANALYSIS = "security_analysis"

class UserRole(Enum):
    """User roles for collaboration"""
    ADMIN = "admin"
    PROJECT_MANAGER = "project_manager"
    DBA = "dba"
    DEVELOPER = "developer"
    VIEWER = "viewer"

class FixCategory(Enum):
    """Categories of auto-fixes"""
    SYNTAX = "syntax"
    PERFORMANCE = "performance"
    SECURITY = "security"
    COMPATIBILITY = "compatibility"
    OPTIMIZATION = "optimization"
    COMPLIANCE = "compliance"

class FixSeverity(Enum):
    """Severity levels for fixes"""
    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"
    COSMETIC = "cosmetic"

class FixStatus(Enum):
    """Status of auto-fix application"""
    PENDING = "pending"
    APPLIED = "applied"
    FAILED = "failed"
    SKIPPED = "skipped"
    REVIEWED = "reviewed"

@dataclass
class User:
    """User representation for collaboration"""
    id: str
    username: str
    email: str
    role: UserRole
    last_active: datetime
    projects: List[str]

@dataclass
class MigrationProject:
    """Migration project representation"""
    id: str
    name: str
    source_engine: str
    target_engine: str
    status: str
    owner_id: str
    team_members: List[str]
    created_at: datetime
    updated_at: datetime
    metadata: Dict

@dataclass
class CostEstimate:
    """Cost estimation result"""
    service: str
    monthly_cost: float
    annual_cost: float
    cost_factors: Dict[str, float]
    optimizations: List[str]
    confidence_score: float

@dataclass
class SecurityAssessment:
    """Security assessment result"""
    overall_score: float
    vulnerabilities: List[Dict]
    compliance_status: Dict[str, bool]
    recommendations: List[str]
    data_classification: Dict[str, str]

@dataclass
class AIAnalysisResult:
    """AI analysis result"""
    analysis_type: AnalysisType
    confidence_score: float
    recommendations: List[str]
    risks: List[Dict[str, str]]
    opportunities: List[str]
    timeline_estimate: str
    cost_impact: str
    detailed_analysis: str
    action_items: List[Dict[str, str]]

@dataclass
class AutoFix:
    """Represents a single auto-fix"""
    id: str
    category: FixCategory
    severity: FixSeverity
    title: str
    description: str
    original_code: str
    fixed_code: str
    explanation: str
    confidence_score: float
    estimated_impact: str
    prerequisites: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)
    status: FixStatus = FixStatus.PENDING
    auto_apply: bool = False

@dataclass
class AutoFixResult:
    """Results of auto-fix analysis"""
    total_issues: int
    fixes_available: int
    fixes_applied: int
    critical_issues: int
    performance_gains: str
    security_improvements: List[str]
    compatibility_score_before: float
    compatibility_score_after: float
    fixes: List[AutoFix]
    summary_report: str

# Complete Database Configuration with Enhanced Features
DATABASE_CONFIG = {
    'mysql': {
        'display_name': 'MySQL',
        'icon': 'üê¨',
        'schema_label': 'MySQL Schema Definition',
        'query_label': 'MySQL Queries',
        'schema_term': 'Database Schema',
        'query_term': 'SQL Queries',
        'file_extensions': ['.sql', '.dump'],
        'aws_target_options': ['aurora_mysql', 'rds_mysql'],
        'enterprise_features': ['High Availability', 'Read Replicas', 'Automated Backups'],
        'sample_schema': '''-- MySQL E-commerce Database Schema Example
-- This demonstrates common MySQL features and patterns

CREATE TABLE users (
    id INT AUTO_INCREMENT PRIMARY KEY,
    username VARCHAR(50) NOT NULL UNIQUE,
    email VARCHAR(100) NOT NULL UNIQUE,
    password_hash VARCHAR(255) NOT NULL,
    first_name VARCHAR(50),
    last_name VARCHAR(50),
    phone VARCHAR(20),
    date_of_birth DATE,
    is_active TINYINT(1) DEFAULT 1,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    INDEX idx_username (username),
    INDEX idx_email (email),
    INDEX idx_created_at (created_at)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

CREATE TABLE categories (
    id INT AUTO_INCREMENT PRIMARY KEY,
    name VARCHAR(100) NOT NULL,
    description TEXT,
    parent_id INT NULL,
    slug VARCHAR(100) NOT NULL UNIQUE,
    is_active TINYINT(1) DEFAULT 1,
    sort_order INT DEFAULT 0,
    FOREIGN KEY (parent_id) REFERENCES categories(id) ON DELETE SET NULL,
    INDEX idx_parent_id (parent_id),
    INDEX idx_slug (slug)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;''',
        'sample_queries': '''-- MySQL Query Examples for E-commerce Application

-- 1. Complex JOIN with aggregation and date functions
SELECT 
    u.username,
    u.email,
    COUNT(o.id) as total_orders,
    SUM(o.total_amount) as total_spent,
    AVG(o.total_amount) as avg_order_value,
    MAX(o.created_at) as last_order_date,
    DATEDIFF(NOW(), MAX(o.created_at)) as days_since_last_order
FROM users u
LEFT JOIN orders o ON u.id = o.user_id AND o.status != 'cancelled'
WHERE u.created_at >= DATE_SUB(NOW(), INTERVAL 1 YEAR)
GROUP BY u.id, u.username, u.email
HAVING total_orders > 0
ORDER BY total_spent DESC
LIMIT 100;'''
    },
    'postgresql': {
        'display_name': 'PostgreSQL',
        'icon': 'üêò',
        'schema_label': 'PostgreSQL Schema Definition',
        'query_label': 'PostgreSQL Queries',
        'schema_term': 'Database Schema',
        'query_term': 'SQL Queries',
        'file_extensions': ['.sql', '.psql'],
        'aws_target_options': ['aurora_postgresql', 'rds_postgresql'],
        'enterprise_features': ['JSONB Support', 'Full Text Search', 'Advanced Indexing'],
        'sample_schema': '''-- PostgreSQL E-commerce Database Schema Example
-- Demonstrates PostgreSQL-specific features and best practices

CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pg_trgm";

CREATE TYPE user_status AS ENUM ('active', 'inactive', 'suspended', 'pending_verification');
CREATE TYPE order_status AS ENUM ('pending', 'processing', 'shipped', 'delivered', 'cancelled', 'returned');

CREATE TABLE users (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    username VARCHAR(50) NOT NULL UNIQUE,
    email VARCHAR(100) NOT NULL UNIQUE,
    password_hash VARCHAR(255) NOT NULL,
    first_name VARCHAR(50),
    last_name VARCHAR(50),
    phone VARCHAR(20),
    date_of_birth DATE,
    status user_status DEFAULT 'pending_verification',
    preferences JSONB DEFAULT '{}',
    last_login_at TIMESTAMP WITH TIME ZONE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    CONSTRAINT check_email_format CHECK (email ~* '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}$')
);''',
        'sample_queries': '''-- PostgreSQL Query Examples with Advanced Features

-- 1. Advanced analytics with window functions and CTEs
WITH monthly_sales AS (
    SELECT 
        DATE_TRUNC('month', created_at) as month,
        COUNT(*) as order_count,
        SUM(total_amount) as revenue,
        AVG(total_amount) as avg_order_value
    FROM orders
    WHERE status IN ('delivered', 'shipped')
        AND created_at >= CURRENT_DATE - INTERVAL '24 months'
    GROUP BY DATE_TRUNC('month', created_at)
)
SELECT 
    TO_CHAR(month, 'YYYY-MM') as sales_month,
    order_count,
    revenue,
    avg_order_value,
    LAG(revenue) OVER (ORDER BY month) as prev_month_revenue
FROM monthly_sales
ORDER BY month DESC;'''
    },
    'oracle': {
        'display_name': 'Oracle Database',
        'icon': 'üî¥',
        'schema_label': 'Oracle Schema Definition',
        'query_label': 'Oracle SQL/PL-SQL',
        'schema_term': 'Oracle Schema',
        'query_term': 'Oracle Queries',
        'file_extensions': ['.sql', '.ora', '.pls'],
        'aws_target_options': ['rds_oracle', 'aurora_postgresql'],
        'enterprise_features': ['Advanced Analytics', 'Partitioning', 'PL/SQL', 'Advanced Security'],
        'sample_schema': '''-- Oracle Database Schema Example
-- Demonstrates Oracle-specific features

CREATE SEQUENCE user_seq START WITH 1 INCREMENT BY 1;

CREATE TABLE users (
    id NUMBER DEFAULT user_seq.NEXTVAL PRIMARY KEY,
    username VARCHAR2(50) NOT NULL UNIQUE,
    email VARCHAR2(100) NOT NULL UNIQUE,
    password_hash VARCHAR2(255) NOT NULL,
    first_name VARCHAR2(50),
    last_name VARCHAR2(50),
    phone VARCHAR2(20),
    date_of_birth DATE,
    is_active NUMBER(1) DEFAULT 1 CHECK (is_active IN (0,1)),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_users_username ON users(username);
CREATE INDEX idx_users_email ON users(email);

-- Oracle trigger for updated_at
CREATE OR REPLACE TRIGGER users_updated_at_trigger
    BEFORE UPDATE ON users
    FOR EACH ROW
BEGIN
    :NEW.updated_at := CURRENT_TIMESTAMP;
END;''',
        'sample_queries': '''-- Oracle Query Examples with Advanced Features

-- 1. Hierarchical query with CONNECT BY
SELECT LEVEL, 
       LPAD(' ', 2*(LEVEL-1)) || category_name as indented_name,
       category_id,
       parent_category_id
FROM categories
START WITH parent_category_id IS NULL
CONNECT BY PRIOR category_id = parent_category_id
ORDER SIBLINGS BY category_name;

-- 2. Advanced analytics with PARTITION BY
SELECT username,
       email,
       order_date,
       order_amount,
       SUM(order_amount) OVER (PARTITION BY username ORDER BY order_date
                              ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as running_total,
       ROW_NUMBER() OVER (PARTITION BY username ORDER BY order_date DESC) as order_rank
FROM users u
JOIN orders o ON u.id = o.user_id
WHERE o.status = 'COMPLETED';'''
    },
    'sql_server': {
        'display_name': 'SQL Server',
        'icon': 'üè¢',
        'schema_label': 'SQL Server Schema Definition',
        'query_label': 'T-SQL Queries',
        'schema_term': 'SQL Server Schema',
        'query_term': 'T-SQL Queries',
        'file_extensions': ['.sql', '.tsql'],
        'aws_target_options': ['rds_sqlserver', 'aurora_postgresql'],
        'enterprise_features': ['Integration Services', 'Analysis Services', 'Reporting Services'],
        'sample_schema': '''-- SQL Server Schema Example
-- Demonstrates SQL Server-specific features

CREATE TABLE users (
    id INT IDENTITY(1,1) PRIMARY KEY,
    username NVARCHAR(50) NOT NULL UNIQUE,
    email NVARCHAR(100) NOT NULL UNIQUE,
    password_hash NVARCHAR(255) NOT NULL,
    first_name NVARCHAR(50),
    last_name NVARCHAR(50),
    phone NVARCHAR(20),
    date_of_birth DATE,
    is_active BIT DEFAULT 1,
    created_at DATETIME2 DEFAULT GETDATE(),
    updated_at DATETIME2 DEFAULT GETDATE()
);

CREATE INDEX IX_users_username ON users(username);
CREATE INDEX IX_users_email ON users(email);

-- SQL Server trigger for updated_at
CREATE TRIGGER tr_users_updated_at
ON users
AFTER UPDATE
AS
BEGIN
    SET NOCOUNT ON;
    UPDATE users 
    SET updated_at = GETDATE()
    FROM users u
    INNER JOIN inserted i ON u.id = i.id;
END;''',
        'sample_queries': '''-- SQL Server T-SQL Examples

-- 1. Common Table Expression with ranking
WITH UserOrderSummary AS (
    SELECT 
        u.username,
        u.email,
        COUNT(o.id) as total_orders,
        SUM(o.total_amount) as total_spent,
        ROW_NUMBER() OVER (ORDER BY SUM(o.total_amount) DESC) as spending_rank
    FROM users u
    LEFT JOIN orders o ON u.id = o.user_id
    WHERE u.created_at >= DATEADD(YEAR, -1, GETDATE())
    GROUP BY u.username, u.email
)
SELECT TOP 100 *
FROM UserOrderSummary
WHERE total_orders > 0
ORDER BY spending_rank;

-- 2. SQL Server window functions
SELECT 
    username,
    order_date,
    order_amount,
    LAG(order_amount) OVER (PARTITION BY username ORDER BY order_date) as prev_order_amount,
    LEAD(order_amount) OVER (PARTITION BY username ORDER BY order_date) as next_order_amount
FROM users u
INNER JOIN orders o ON u.id = o.user_id;'''
    },
    'mongodb': {
        'display_name': 'MongoDB',
        'icon': 'üçÉ',
        'schema_label': 'MongoDB Collection Schema',
        'query_label': 'MongoDB Queries',
        'schema_term': 'Document Schema',
        'query_term': 'MongoDB Queries',
        'file_extensions': ['.js', '.json'],
        'aws_target_options': ['documentdb', 'aurora_postgresql'],
        'enterprise_features': ['Document Store', 'Horizontal Scaling', 'Flexible Schema', 'Aggregation Pipeline'],
        'sample_schema': '''// MongoDB Collection Schema Example
// Demonstrates MongoDB document structure and indexing

// Users Collection
db.users.createIndex({ "username": 1 }, { unique: true });
db.users.createIndex({ "email": 1 }, { unique: true });
db.users.createIndex({ "createdAt": 1 });
db.users.createIndex({ "profile.location": "2dsphere" });

// Sample document structure
{
  "_id": ObjectId("..."),
  "username": "john_doe",
  "email": "john@example.com",
  "passwordHash": "...",
  "profile": {
    "firstName": "John",
    "lastName": "Doe",
    "dateOfBirth": ISODate("1990-01-15"),
    "phone": "+1234567890",
    "address": {
      "street": "123 Main St",
      "city": "New York",
      "state": "NY",
      "zipCode": "10001",
      "country": "USA"
    },
    "preferences": {
      "newsletter": true,
      "notifications": {
        "email": true,
        "sms": false
      }
    }
  },
  "isActive": true,
  "createdAt": ISODate("2024-01-15T10:30:00Z"),
  "updatedAt": ISODate("2024-01-15T10:30:00Z")
}''',
        'sample_queries': '''// MongoDB Query Examples with Aggregation Pipeline

// 1. Complex aggregation with user order summary
db.users.aggregate([
  {
    $lookup: {
      from: "orders",
      localField: "_id",
      foreignField: "userId",
      as: "orders"
    }
  },
  {
    $match: {
      "createdAt": { $gte: new Date("2023-01-01") }
    }
  },
  {
    $project: {
      username: 1,
      email: 1,
      totalOrders: { $size: "$orders" },
      totalSpent: {
        $sum: {
          $map: {
            input: "$orders",
            as: "order",
            in: "$$order.totalAmount"
          }
        }
      },
      avgOrderValue: {
        $avg: {
          $map: {
            input: "$orders",
            as: "order", 
            in: "$$order.totalAmount"
          }
        }
      }
    }
  },
  {
    $match: {
      totalOrders: { $gt: 0 }
    }
  },
  {
    $sort: { totalSpent: -1 }
  },
  {
    $limit: 100
  }
]);

// 2. Geospatial query for nearby users
db.users.find({
  "profile.location": {
    $near: {
      $geometry: {
        type: "Point",
        coordinates: [-73.9857, 40.7484] // NYC coordinates
      },
      $maxDistance: 5000 // 5km radius
    }
  }
});'''
    }
}

# Initialize Session State for Enterprise Features
def initialize_session_state():
    """Initialize session state for enterprise features"""
    defaults = {
        'user_id': str(uuid.uuid4()),
        'current_project': None,
        'projects': [],
        'analysis_results': {},
        'cost_estimates': {},
        'security_assessment': None,
        'collaboration_enabled': False,
        'autofix_results': None,
        'example_schema': '',
        'example_source': '',
        'example_target': '',
        'show_project_creator': False
    }
    
    for key, default_value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = default_value

# Database Manager for Enterprise Features
class EnterpriseDBManager:
    """Manage enterprise database operations"""
    
    def __init__(self):
        self.db_path = Path("enterprise_migration.db")
        self.init_database()
    
    def init_database(self):
        """Initialize SQLite database for enterprise features"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Users table
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS users (
                    id TEXT PRIMARY KEY,
                    username TEXT UNIQUE NOT NULL,
                    email TEXT UNIQUE NOT NULL,
                    role TEXT NOT NULL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    last_active TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    metadata TEXT DEFAULT '{}'
                )
            ''')
            
            # Projects table
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS migration_projects (
                    id TEXT PRIMARY KEY,
                    name TEXT NOT NULL,
                    source_engine TEXT NOT NULL,
                    target_engine TEXT NOT NULL,
                    status TEXT DEFAULT 'planning',
                    owner_id TEXT NOT NULL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    metadata TEXT DEFAULT '{}'
                )
            ''')
            
            # Analysis results table
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS analysis_results (
                    id TEXT PRIMARY KEY,
                    project_id TEXT NOT NULL,
                    analysis_type TEXT NOT NULL,
                    result_data TEXT NOT NULL,
                    confidence_score REAL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (project_id) REFERENCES migration_projects (id)
                )
            ''')
            
            conn.commit()
            conn.close()
        except Exception as e:
            logger.error(f"Database initialization failed: {e}")
    
    def create_project(self, name: str, source_engine: str, target_engine: str, owner_id: str) -> str:
        """Create new migration project"""
        try:
            project_id = str(uuid.uuid4())
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO migration_projects (id, name, source_engine, target_engine, owner_id)
                VALUES (?, ?, ?, ?, ?)
            ''', (project_id, name, source_engine, target_engine, owner_id))
            
            conn.commit()
            conn.close()
            return project_id
        except Exception as e:
            logger.error(f"Project creation failed: {e}")
            return str(uuid.uuid4())  # Fallback
    
    def get_user_projects(self, user_id: str) -> List[Dict]:
        """Get projects for user"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT id, name, source_engine, target_engine, status, created_at
                FROM migration_projects
                WHERE owner_id = ?
                ORDER BY created_at DESC
            ''', (user_id,))
            
            projects = []
            for row in cursor.fetchall():
                projects.append({
                    'id': row[0],
                    'name': row[1],
                    'source_engine': row[2],
                    'target_engine': row[3],
                    'status': row[4],
                    'created_at': row[5]
                })
            
            conn.close()
            return projects
        except Exception as e:
            logger.error(f"Failed to get user projects: {e}")
            return []

# Enhanced AWS Cost Calculator
class EnhancedAWSCostCalculator:
    """Enhanced AWS cost calculation with real-time pricing"""
    
    def __init__(self):
        self.connected = False
        try:
            # Try to import and initialize AWS clients
            import boto3
            self.pricing_client = boto3.client('pricing', region_name='us-east-1')
            self.ce_client = boto3.client('ce')
            self.rds_client = boto3.client('rds')
            self.connected = True
            logger.info("AWS Cost Calculator initialized successfully")
        except Exception as e:
            self.connected = False
            logger.warning(f"AWS Cost Calculator using fallback pricing: {e}")
    
    def estimate_total_migration_cost(self, config: Dict) -> CostEstimate:
        """Estimate total migration cost including all components"""
        try:
            # Calculate individual components
            rds_cost = self._calculate_rds_cost(config)
            storage_cost = self._calculate_storage_cost(config)
            backup_cost = self._calculate_backup_cost(config)
            dms_cost = self._calculate_dms_cost(config)
            data_transfer_cost = self._calculate_data_transfer_cost(config)
            
            total_monthly = rds_cost + storage_cost + backup_cost
            total_annual = total_monthly * 12
            migration_cost = dms_cost + data_transfer_cost
            
            # Generate optimizations
            optimizations = self._generate_cost_optimizations(config, total_monthly)
            
            return CostEstimate(
                service="Complete Migration",
                monthly_cost=total_monthly,
                annual_cost=total_annual,
                cost_factors={
                    'rds_instance': rds_cost,
                    'storage': storage_cost,
                    'backup': backup_cost,
                    'migration': migration_cost,
                    'data_transfer': data_transfer_cost
                },
                optimizations=optimizations,
                confidence_score=0.85 if self.connected else 0.65
            )
            
        except Exception as e:
            logger.error(f"Cost estimation failed: {e}")
            return self._get_fallback_estimate()
    
    def _calculate_rds_cost(self, config: Dict) -> float:
        """Calculate RDS instance cost"""
        # Simplified calculation - in production use actual API
        instance_costs = {
            'db.t3.micro': 16.79,
            'db.t3.small': 33.58,
            'db.t3.medium': 67.15,
            'db.t3.large': 134.30,
            'db.m5.large': 175.20,
            'db.m5.xlarge': 350.40,
            'db.r5.large': 217.44,
            'db.r5.xlarge': 434.88
        }
        
        instance_class = config.get('instance_class', 'db.t3.medium')
        base_cost = instance_costs.get(instance_class, 67.15)
        
        # Apply multi-AZ multiplier
        if config.get('multi_az', False):
            base_cost *= 2
        
        # Apply Aurora premium
        if 'aurora' in config.get('target_engine', ''):
            base_cost *= 1.2
        
        return base_cost
    
    def _calculate_storage_cost(self, config: Dict) -> float:
        """Calculate storage cost"""
        storage_gb = config.get('storage_gb', 100)
        storage_type = config.get('storage_type', 'gp2')
        
        storage_rates = {
            'gp2': 0.115,
            'gp3': 0.08,
            'io1': 0.125,
            'io2': 0.125
        }
        
        return storage_gb * storage_rates.get(storage_type, 0.115)
    
    def _calculate_backup_cost(self, config: Dict) -> float:
        """Calculate backup storage cost"""
        storage_gb = config.get('storage_gb', 100)
        backup_retention = config.get('backup_retention_days', 7)
        
        # Backup storage is typically 20-50% of primary storage
        backup_multiplier = min(backup_retention / 7 * 0.3, 1.0)
        return storage_gb * backup_multiplier * 0.095  # Backup storage rate
    
    def _calculate_dms_cost(self, config: Dict) -> float:
        """Calculate DMS migration cost"""
        dms_instance_costs = {
            'dms.t3.micro': 18.0,
            'dms.t3.small': 36.0,
            'dms.t3.medium': 72.0,
            'dms.t3.large': 144.0
        }
        
        dms_instance = config.get('dms_instance', 'dms.t3.medium')
        migration_hours = config.get('migration_duration_hours', 24)
        
        hourly_cost = dms_instance_costs.get(dms_instance, 72.0) / (24 * 30)  # Convert monthly to hourly
        return hourly_cost * migration_hours
    
    def _calculate_data_transfer_cost(self, config: Dict) -> float:
        """Calculate data transfer cost"""
        data_size_gb = config.get('data_size_gb', 100)
        
        if data_size_gb <= 1:
            return 0
        elif data_size_gb <= 10240:  # Up to 10TB
            return (data_size_gb - 1) * 0.09
        else:
            return (10239 * 0.09) + ((data_size_gb - 10240) * 0.085)
    
    def _generate_cost_optimizations(self, config: Dict, monthly_cost: float) -> List[str]:
        """Generate cost optimization recommendations"""
        optimizations = []
        
        # Instance optimization
        if monthly_cost > 500:
            optimizations.append("Consider Reserved Instances for 40-60% savings")
        
        # Storage optimization
        if config.get('storage_type', 'gp2') == 'gp2':
            optimizations.append("Upgrade to gp3 storage for 30% cost savings")
        
        # Multi-AZ optimization
        if not config.get('multi_az', False):
            optimizations.append("Enable Multi-AZ for high availability")
        
        # Aurora optimization
        if 'aurora' not in config.get('target_engine', ''):
            optimizations.append("Consider Aurora for better performance/cost ratio")
        
        return optimizations
    
    def _get_fallback_estimate(self) -> CostEstimate:
        """Fallback estimate when AWS API is unavailable"""
        return CostEstimate(
            service="Estimated Migration Cost",
            monthly_cost=250.0,
            annual_cost=3000.0,
            cost_factors={
                'rds_instance': 150.0,
                'storage': 50.0,
                'backup': 25.0,
                'migration': 100.0
            },
            optimizations=["Enable AWS API for precise cost calculation"],
            confidence_score=0.5
        )

# Enhanced AI Analyzer with Multiple Analysis Types
class EnterpriseAIAnalyzer:
    """Enterprise-grade AI analyzer with comprehensive migration analysis"""
    
    def __init__(self):
        self.client = None
        self.connected = False
        
        try:
            # Try to import anthropic and initialize
            import anthropic
            api_key = st.secrets.get("ANTHROPIC_API_KEY")
            if api_key:
                self.client = anthropic.Anthropic(api_key=api_key)
                self.connected = True
                logger.info("Enterprise AI Analyzer initialized successfully")
            else:
                logger.warning("ANTHROPIC_API_KEY not found in secrets")
        except Exception as e:
            logger.warning(f"AI Analyzer using mock mode: {e}")
    
    async def comprehensive_analysis(self, migration_context: Dict) -> Dict[AnalysisType, AIAnalysisResult]:
        """Run comprehensive AI analysis across all aspects"""
        if not self.connected:
            return self._get_fallback_analysis()
        
        analyses = {}
        
        try:
            # Run different analysis types
            strategy_analysis = await self._analyze_migration_strategy(migration_context)
            risk_analysis = await self._analyze_risks(migration_context)
            cost_analysis = await self._analyze_cost_optimization(migration_context)
            timeline_analysis = await self._analyze_timeline(migration_context)
            security_analysis = await self._analyze_security(migration_context)
            
            analyses[AnalysisType.MIGRATION_STRATEGY] = strategy_analysis
            analyses[AnalysisType.RISK_ASSESSMENT] = risk_analysis
            analyses[AnalysisType.COST_OPTIMIZATION] = cost_analysis
            analyses[AnalysisType.TIMELINE_ESTIMATION] = timeline_analysis
            analyses[AnalysisType.SECURITY_ANALYSIS] = security_analysis
            
            return analyses
            
        except Exception as e:
            logger.error(f"Comprehensive analysis failed: {e}")
            return self._get_fallback_analysis()
    
    async def _analyze_migration_strategy(self, context: Dict) -> AIAnalysisResult:
        """Analyze migration strategy"""
        if not self.connected:
            return self._get_fallback_result(AnalysisType.MIGRATION_STRATEGY)
            
        # Mock implementation for demo
        await asyncio.sleep(0.5)  # Simulate processing
        
        return AIAnalysisResult(
            analysis_type=AnalysisType.MIGRATION_STRATEGY,
            confidence_score=0.85,
            recommendations=[
                "Use AWS DMS for minimal downtime migration",
                "Implement Aurora PostgreSQL for better performance",
                "Set up read replicas for load distribution",
                "Configure automated backups and point-in-time recovery"
            ],
            risks=[
                {"description": "Data type compatibility issues", "severity": "medium", "category": "technical"},
                {"description": "Application downtime during cutover", "severity": "high", "category": "business"}
            ],
            opportunities=[
                "Improved performance with Aurora",
                "Cost savings with reserved instances",
                "Enhanced monitoring with CloudWatch"
            ],
            timeline_estimate="6-8 weeks",
            cost_impact="20-30% cost reduction expected",
            detailed_analysis="Comprehensive migration analysis indicates high feasibility with proper planning and execution.",
            action_items=[
                {"action": "Set up AWS DMS replication instance", "priority": "high", "category": "setup"},
                {"action": "Test schema conversion", "priority": "medium", "category": "validation"}
            ]
        )
    
    async def _analyze_risks(self, context: Dict) -> AIAnalysisResult:
        """Comprehensive risk analysis"""
        if not self.connected:
            return self._get_fallback_result(AnalysisType.RISK_ASSESSMENT)
            
        # Mock implementation for demo
        await asyncio.sleep(0.5)  # Simulate processing
        
        return AIAnalysisResult(
            analysis_type=AnalysisType.RISK_ASSESSMENT,
            confidence_score=0.78,
            recommendations=[
                "Perform comprehensive testing in staging environment",
                "Implement rollback procedures",
                "Monitor application performance post-migration",
                "Set up automated data validation checks"
            ],
            risks=[
                {"description": "Query performance degradation", "severity": "medium", "category": "performance"},
                {"description": "Data consistency issues", "severity": "high", "category": "data"},
                {"description": "Application compatibility problems", "severity": "medium", "category": "technical"}
            ],
            opportunities=[
                "Implement better monitoring",
                "Optimize query performance",
                "Enhance security posture"
            ],
            timeline_estimate="Risk mitigation: 2-3 weeks",
            cost_impact="Risk mitigation budget: 10-15% of project cost",
            detailed_analysis="Risk assessment reveals manageable migration risks with proper mitigation strategies.",
            action_items=[
                {"action": "Create comprehensive test plan", "priority": "high", "category": "testing"},
                {"action": "Set up monitoring alerts", "priority": "medium", "category": "monitoring"}
            ]
        )
    
    async def _analyze_cost_optimization(self, context: Dict) -> AIAnalysisResult:
        """AI-powered cost optimization analysis"""
        if not self.connected:
            return self._get_fallback_result(AnalysisType.COST_OPTIMIZATION)
            
        # Mock implementation for demo
        await asyncio.sleep(0.5)  # Simulate processing
        
        return AIAnalysisResult(
            analysis_type=AnalysisType.COST_OPTIMIZATION,
            confidence_score=0.82,
            recommendations=[
                "Use Reserved Instances for 40-60% cost savings",
                "Implement Aurora Serverless for variable workloads",
                "Optimize storage with gp3 volumes",
                "Set up automated scaling policies"
            ],
            risks=[
                {"description": "Over-provisioning resources", "severity": "medium", "category": "cost"},
                {"description": "Under-estimating data transfer costs", "severity": "low", "category": "cost"}
            ],
            opportunities=[
                "Significant cost reduction with cloud-native features",
                "Improved resource utilization",
                "Better cost predictability"
            ],
            timeline_estimate="Cost optimization: Immediate to 6 months",
            cost_impact="Estimated 25-40% cost reduction",
            detailed_analysis="Cost optimization analysis shows significant savings opportunities with proper AWS service selection.",
            action_items=[
                {"action": "Implement Reserved Instance strategy", "priority": "high", "category": "cost"},
                {"action": "Set up cost monitoring alerts", "priority": "medium", "category": "monitoring"}
            ]
        )
    
    async def _analyze_timeline(self, context: Dict) -> AIAnalysisResult:
        """AI-powered timeline estimation"""
        if not self.connected:
            return self._get_fallback_result(AnalysisType.TIMELINE_ESTIMATION)
            
        # Mock implementation for demo
        await asyncio.sleep(0.5)  # Simulate processing
        
        return AIAnalysisResult(
            analysis_type=AnalysisType.TIMELINE_ESTIMATION,
            confidence_score=0.75,
            recommendations=[
                "Plan for 8-12 week total migration timeline",
                "Allocate 20% buffer for unexpected issues",
                "Implement parallel testing and migration phases",
                "Set up milestone checkpoints for progress tracking"
            ],
            risks=[
                {"description": "Schema complexity delays", "severity": "medium", "category": "timeline"},
                {"description": "Testing phase extensions", "severity": "medium", "category": "timeline"}
            ],
            opportunities=[
                "Accelerate with automated tools",
                "Parallel processing opportunities",
                "Early Go-Live for non-critical components"
            ],
            timeline_estimate="8-12 weeks total project timeline",
            cost_impact="Timeline optimization can reduce costs by 15-20%",
            detailed_analysis="Timeline analysis indicates realistic 8-12 week migration with proper resource allocation.",
            action_items=[
                {"action": "Create detailed project timeline", "priority": "high", "category": "planning"},
                {"action": "Allocate adequate testing time", "priority": "high", "category": "planning"}
            ]
        )
    
    async def _analyze_security(self, context: Dict) -> AIAnalysisResult:
        """AI-powered security analysis"""
        if not self.connected:
            return self._get_fallback_result(AnalysisType.SECURITY_ANALYSIS)
            
        # Mock implementation for demo
        await asyncio.sleep(0.5)  # Simulate processing
        
        return AIAnalysisResult(
            analysis_type=AnalysisType.SECURITY_ANALYSIS,
            confidence_score=0.88,
            recommendations=[
                "Enable encryption at rest and in transit",
                "Implement IAM database authentication",
                "Set up VPC with private subnets",
                "Configure automated security monitoring"
            ],
            risks=[
                {"description": "Inadequate access controls", "severity": "high", "category": "security"},
                {"description": "Unencrypted data transmission", "severity": "medium", "category": "security"}
            ],
            opportunities=[
                "Enhanced security with AWS security services",
                "Improved compliance posture",
                "Better audit capabilities"
            ],
            timeline_estimate="Security implementation: 2-4 weeks",
            cost_impact="Security infrastructure: 5-10% of total cost",
            detailed_analysis="Security analysis shows need for comprehensive security implementation with AWS best practices.",
            action_items=[
                {"action": "Enable all encryption options", "priority": "high", "category": "security"},
                {"action": "Set up security monitoring", "priority": "high", "category": "security"}
            ]
        )
    
    def _get_fallback_analysis(self) -> Dict[AnalysisType, AIAnalysisResult]:
        """Fallback analysis when AI is not available"""
        fallback = {}
        
        for analysis_type in [AnalysisType.MIGRATION_STRATEGY, AnalysisType.RISK_ASSESSMENT, 
                             AnalysisType.COST_OPTIMIZATION, AnalysisType.TIMELINE_ESTIMATION, 
                             AnalysisType.SECURITY_ANALYSIS]:
            fallback[analysis_type] = self._get_fallback_result(analysis_type)
        
        return fallback
    
    def _get_fallback_result(self, analysis_type: AnalysisType) -> AIAnalysisResult:
        """Fallback result for specific analysis type"""
        return AIAnalysisResult(
            analysis_type=analysis_type,
            confidence_score=0.6,
            recommendations=["AI analysis not available - manual review required"],
            risks=[{"description": "Unable to perform AI assessment", "severity": "medium", "category": "technical"}],
            opportunities=["Manual analysis may reveal additional opportunities"],
            timeline_estimate="Manual estimation required",
            cost_impact="Manual cost analysis required",
            detailed_analysis="AI analysis service not available. Please perform manual analysis.",
            action_items=[{"action": "Enable AI analysis service", "priority": "high", "category": "setup"}]
        )

# Security Analyzer
class SecurityAnalyzer:
    """Comprehensive security analysis for database migration"""
    
    def __init__(self):
        self.compliance_frameworks = {
            'GDPR': ['data_encryption', 'access_controls', 'audit_logging', 'data_retention'],
            'HIPAA': ['data_encryption', 'access_controls', 'audit_logging', 'data_backup'],
            'SOC2': ['security_monitoring', 'access_controls', 'change_management', 'incident_response'],
            'PCI_DSS': ['data_encryption', 'network_security', 'access_controls', 'regular_monitoring']
        }
    
    def analyze_security(self, migration_context: Dict) -> SecurityAssessment:
        """Perform comprehensive security analysis"""
        try:
            # Analyze data classification
            data_classification = self._classify_data(migration_context.get('schema_ddl', ''))
            
            # Check compliance requirements
            compliance_status = self._check_compliance(migration_context, data_classification)
            
            # Identify vulnerabilities
            vulnerabilities = self._identify_vulnerabilities(migration_context)
            
            # Generate recommendations
            recommendations = self._generate_security_recommendations(
                migration_context, data_classification, vulnerabilities
            )
            
            # Calculate overall security score
            security_score = self._calculate_security_score(compliance_status, vulnerabilities)
            
            return SecurityAssessment(
                overall_score=security_score,
                vulnerabilities=vulnerabilities,
                compliance_status=compliance_status,
                recommendations=recommendations,
                data_classification=data_classification
            )
            
        except Exception as e:
            logger.error(f"Security analysis failed: {e}")
            return self._get_fallback_security_assessment()
    
    def _classify_data(self, schema_ddl: str) -> Dict[str, str]:
        """Classify data types for security assessment"""
        classification = {}
        
        pii_patterns = {
            'email': r'email|e_mail|electronic_mail',
            'phone': r'phone|telephone|mobile|cell',
            'ssn': r'ssn|social_security|social_security_number',
            'credit_card': r'credit_card|cc_number|card_number',
            'address': r'address|street|zip|postal',
            'name': r'first_name|last_name|full_name|surname'
        }
        
        for table_match in re.finditer(r'CREATE TABLE (\w+)', schema_ddl, re.IGNORECASE):
            table_name = table_match.group(1)
            
            # Check for PII patterns in column definitions
            table_section = schema_ddl[table_match.start():].split(';')[0]
            
            classification[table_name] = 'public'  # Default
            
            for pii_type, pattern in pii_patterns.items():
                if re.search(pattern, table_section, re.IGNORECASE):
                    classification[table_name] = 'pii'
                    break
            
            # Check for financial or healthcare indicators
            if re.search(r'payment|transaction|billing|medical|health', table_section, re.IGNORECASE):
                classification[table_name] = 'sensitive'
        
        return classification
    
    def _check_compliance(self, context: Dict, data_classification: Dict) -> Dict[str, bool]:
        """Check compliance framework requirements"""
        compliance_status = {}
        required_frameworks = context.get('compliance_requirements', ['GDPR'])
        
        for framework in required_frameworks:
            if framework in self.compliance_frameworks:
                requirements = self.compliance_frameworks[framework]
                
                # Check if requirements are met (simplified check)
                met_requirements = 0
                total_requirements = len(requirements)
                
                # Encryption check
                if 'data_encryption' in requirements:
                    if context.get('encryption_at_rest', False):
                        met_requirements += 1
                
                # Access controls check
                if 'access_controls' in requirements:
                    if context.get('iam_enabled', False):
                        met_requirements += 1
                
                # Audit logging check
                if 'audit_logging' in requirements:
                    if context.get('audit_logging', False):
                        met_requirements += 1
                
                compliance_status[framework] = (met_requirements / total_requirements) >= 0.8
        
        return compliance_status
    
    def _identify_vulnerabilities(self, context: Dict) -> List[Dict]:
        """Identify potential security vulnerabilities"""
        vulnerabilities = []
        
        # Check encryption
        if not context.get('encryption_at_rest', False):
            vulnerabilities.append({
                'type': 'encryption',
                'severity': 'high',
                'description': 'Data at rest encryption not enabled',
                'recommendation': 'Enable encryption at rest for all database storage'
            })
        
        if not context.get('encryption_in_transit', False):
            vulnerabilities.append({
                'type': 'encryption',
                'severity': 'high',
                'description': 'Data in transit encryption not configured',
                'recommendation': 'Enable SSL/TLS encryption for all database connections'
            })
        
        # Check network security
        if not context.get('vpc_enabled', False):
            vulnerabilities.append({
                'type': 'network',
                'severity': 'medium',
                'description': 'Database not in private VPC',
                'recommendation': 'Deploy database in private VPC with proper security groups'
            })
        
        # Check access controls
        if not context.get('iam_enabled', False):
            vulnerabilities.append({
                'type': 'access_control',
                'severity': 'medium',
                'description': 'IAM database authentication not enabled',
                'recommendation': 'Enable IAM database authentication for enhanced security'
            })
        
        return vulnerabilities
    
    def _generate_security_recommendations(self, context: Dict, 
                                         data_classification: Dict, 
                                         vulnerabilities: List[Dict]) -> List[str]:
        """Generate security recommendations"""
        recommendations = []
        
        # PII handling recommendations
        if any(classification == 'pii' for classification in data_classification.values()):
            recommendations.extend([
                "Implement data masking for non-production environments",
                "Enable detailed audit logging for PII access",
                "Configure automatic PII detection and classification"
            ])
        
        # Compliance recommendations
        compliance_requirements = context.get('compliance_requirements', [])
        if 'GDPR' in compliance_requirements:
            recommendations.extend([
                "Implement data retention policies",
                "Enable right-to-be-forgotten procedures",
                "Document data processing activities"
            ])
        
        # Vulnerability-based recommendations
        for vuln in vulnerabilities:
            if vuln['severity'] == 'high':
                recommendations.append(f"HIGH PRIORITY: {vuln['recommendation']}")
        
        return recommendations[:8]  # Limit to top 8
    
    def _calculate_security_score(self, compliance_status: Dict, vulnerabilities: List[Dict]) -> float:
        """Calculate overall security score"""
        base_score = 100.0
        
        # Deduct for vulnerabilities
        for vuln in vulnerabilities:
            if vuln['severity'] == 'high':
                base_score -= 20
            elif vuln['severity'] == 'medium':
                base_score -= 10
            else:
                base_score -= 5
        
        # Deduct for compliance failures
        total_frameworks = len(compliance_status) if compliance_status else 1
        failed_frameworks = sum(1 for status in compliance_status.values() if not status)
        compliance_penalty = (failed_frameworks / total_frameworks) * 30
        
        base_score -= compliance_penalty
        
        return max(0.0, min(100.0, base_score))
    
    def _get_fallback_security_assessment(self) -> SecurityAssessment:
        """Fallback security assessment"""
        return SecurityAssessment(
            overall_score=70.0,
            vulnerabilities=[{
                'type': 'assessment',
                'severity': 'medium',
                'description': 'Security analysis not available',
                'recommendation': 'Perform manual security assessment'
            }],
            compliance_status={'Manual Review': False},
            recommendations=["Conduct comprehensive security review"],
            data_classification={'unknown': 'manual_review_required'}
        )

class EnterpriseAutoFixEngine:
    """Enterprise-grade auto-fix engine for database migration"""
    
    def __init__(self):
        self.fix_patterns = self._load_fix_patterns()
        self.ai_client = None
        self.connected = False
        
        try:
            import anthropic
            api_key = st.secrets.get("ANTHROPIC_API_KEY")
            if api_key:
                self.ai_client = anthropic.Anthropic(api_key=api_key)
                self.connected = True
                logger.info("AutoFix AI Engine initialized successfully")
        except Exception as e:
            logger.warning(f"AutoFix AI Engine using rule-based mode: {e}")
    
    def analyze_and_fix(self, source_engine: str, target_engine: str, 
                       schema_ddl: str, queries: str = "", 
                       fix_categories: List[FixCategory] = None,
                       auto_apply_safe: bool = False) -> AutoFixResult:
        """Comprehensive analysis and auto-fix generation"""
        
        if fix_categories is None:
            fix_categories = list(FixCategory)
        
        all_fixes = []
        
        try:
            # Schema fixes
            if schema_ddl and FixCategory.SYNTAX in fix_categories:
                schema_fixes = self._analyze_schema_fixes(source_engine, target_engine, schema_ddl)
                all_fixes.extend(schema_fixes)
            
            # Query fixes
            if queries and FixCategory.COMPATIBILITY in fix_categories:
                query_fixes = self._analyze_query_fixes(source_engine, target_engine, queries)
                all_fixes.extend(query_fixes)
            
            # Performance optimization fixes
            if FixCategory.PERFORMANCE in fix_categories:
                perf_fixes = self._analyze_performance_fixes(source_engine, target_engine, schema_ddl, queries)
                all_fixes.extend(perf_fixes)
            
            # Security fixes
            if FixCategory.SECURITY in fix_categories:
                security_fixes = self._analyze_security_fixes(source_engine, target_engine, schema_ddl)
                all_fixes.extend(security_fixes)
            
            # Compliance fixes
            if FixCategory.COMPLIANCE in fix_categories:
                compliance_fixes = self._analyze_compliance_fixes(schema_ddl)
                all_fixes.extend(compliance_fixes)
            
            # AI-enhanced fixes if available
            if self.connected and (schema_ddl or queries):
                try:
                    # Create new event loop for async operations
                    import asyncio
                    if asyncio.get_event_loop().is_running():
                        # If loop is already running, create a task
                        ai_fixes = []
                    else:
                        ai_fixes = asyncio.run(
                            self._get_ai_enhanced_fixes(source_engine, target_engine, schema_ddl, queries)
                        )
                    all_fixes.extend(ai_fixes)
                except Exception as e:
                    logger.warning(f"AI-enhanced fixes failed: {e}")
                    # Continue without AI fixes
            
            # Auto-apply safe fixes if requested
            if auto_apply_safe:
                all_fixes = self._auto_apply_safe_fixes(all_fixes)
            
            # Generate comprehensive result
            result = self._generate_fix_result(all_fixes, schema_ddl, queries)
            
            return result
            
        except Exception as e:
            logger.error(f"Auto-fix analysis failed: {e}")
            return self._get_fallback_fix_result()
    
    def _analyze_schema_fixes(self, source_engine: str, target_engine: str, schema_ddl: str) -> List[AutoFix]:
        """Analyze and generate schema compatibility fixes"""
        fixes = []
        
        # MySQL to PostgreSQL common fixes
        if source_engine == 'mysql' and 'postgresql' in target_engine:
            fixes.extend(self._mysql_to_postgresql_schema_fixes(schema_ddl))
        
        # Oracle to PostgreSQL fixes
        elif source_engine == 'oracle' and 'postgresql' in target_engine:
            fixes.extend(self._oracle_to_postgresql_schema_fixes(schema_ddl))
        
        # SQL Server to PostgreSQL fixes
        elif source_engine == 'sql_server' and 'postgresql' in target_engine:
            fixes.extend(self._sqlserver_to_postgresql_schema_fixes(schema_ddl))
        
        # Generic AWS optimization fixes
        fixes.extend(self._aws_optimization_schema_fixes(schema_ddl, target_engine))
        
        return fixes
    
    def _mysql_to_postgresql_schema_fixes(self, schema_ddl: str) -> List[AutoFix]:
        """MySQL to PostgreSQL specific schema fixes"""
        fixes = []
        
        # AUTO_INCREMENT to SERIAL conversion
        auto_increment_pattern = re.compile(r'(\w+)\s+INT\s+AUTO_INCREMENT', re.IGNORECASE)
        for match in auto_increment_pattern.finditer(schema_ddl):
            original = match.group(0)
            fixed = f"{match.group(1)} SERIAL"
            
            fixes.append(AutoFix(
                id=f"mysql_autoincrement_{len(fixes)}",
                category=FixCategory.SYNTAX,
                severity=FixSeverity.HIGH,
                title="Convert AUTO_INCREMENT to SERIAL",
                description="MySQL AUTO_INCREMENT is not supported in PostgreSQL",
                original_code=original,
                fixed_code=fixed,
                explanation="PostgreSQL uses SERIAL or BIGSERIAL for auto-incrementing columns",
                confidence_score=0.95,
                estimated_impact="Essential for PostgreSQL compatibility",
                auto_apply=True
            ))
        
        # ENUM type conversion
        enum_pattern = re.compile(r'(\w+)\s+ENUM\s*\((.*?)\)', re.IGNORECASE | re.DOTALL)
        for match in enum_pattern.finditer(schema_ddl):
            column_name = match.group(1)
            enum_values = match.group(2)
            
            # Create CHECK constraint version
            check_constraint = f"{column_name} VARCHAR(50) CHECK ({column_name} IN ({enum_values}))"
            
            fixes.append(AutoFix(
                id=f"mysql_enum_{len(fixes)}",
                category=FixCategory.SYNTAX,
                severity=FixSeverity.MEDIUM,
                title="Convert ENUM to CHECK constraint",
                description="MySQL ENUM type converted to VARCHAR with CHECK constraint",
                original_code=match.group(0),
                fixed_code=check_constraint,
                explanation="PostgreSQL handles ENUM differently; CHECK constraint provides similar functionality",
                confidence_score=0.88,
                estimated_impact="Maintains data integrity with PostgreSQL compatibility",
                auto_apply=False,
                warnings=["Review enum values for application compatibility"]
            ))
        
        # TINYINT to SMALLINT conversion
        tinyint_pattern = re.compile(r'\bTINYINT\b', re.IGNORECASE)
        if tinyint_pattern.search(schema_ddl):
            fixes.append(AutoFix(
                id=f"mysql_tinyint_{len(fixes)}",
                category=FixCategory.SYNTAX,
                severity=FixSeverity.MEDIUM,
                title="Convert TINYINT to SMALLINT",
                description="PostgreSQL doesn't have TINYINT, use SMALLINT instead",
                original_code="TINYINT",
                fixed_code="SMALLINT",
                explanation="SMALLINT provides equivalent functionality in PostgreSQL",
                confidence_score=0.92,
                estimated_impact="Direct compatibility improvement",
                auto_apply=True
            ))
        
        # Engine specification removal
        engine_pattern = re.compile(r'ENGINE\s*=\s*\w+', re.IGNORECASE)
        if engine_pattern.search(schema_ddl):
            fixes.append(AutoFix(
                id=f"mysql_engine_{len(fixes)}",
                category=FixCategory.SYNTAX,
                severity=FixSeverity.LOW,
                title="Remove ENGINE specification",
                description="PostgreSQL doesn't use ENGINE specifications",
                original_code="ENGINE=InnoDB",
                fixed_code="",
                explanation="PostgreSQL uses a single storage engine",
                confidence_score=0.98,
                estimated_impact="Clean up unnecessary MySQL-specific syntax",
                auto_apply=True
            ))
        
        return fixes
    
    def _oracle_to_postgresql_schema_fixes(self, schema_ddl: str) -> List[AutoFix]:
        """Oracle to PostgreSQL specific schema fixes"""
        fixes = []
        
        # NUMBER to numeric/integer conversion
        number_pattern = re.compile(r'(\w+)\s+NUMBER(?:\((\d+)(?:,\s*(\d+))?\))?', re.IGNORECASE)
        for match in number_pattern.finditer(schema_ddl):
            column_name = match.group(1)
            precision = match.group(2)
            scale = match.group(3)
            
            if scale and int(scale) > 0:
                # Has decimal places - use NUMERIC
                if precision:
                    fixed = f"{column_name} NUMERIC({precision},{scale})"
                else:
                    fixed = f"{column_name} NUMERIC"
            elif precision and int(precision) <= 9:
                # Integer, fits in INTEGER
                fixed = f"{column_name} INTEGER"
            elif precision and int(precision) <= 18:
                # Larger integer, use BIGINT
                fixed = f"{column_name} BIGINT"
            else:
                # Default to NUMERIC
                fixed = f"{column_name} NUMERIC"
            
            fixes.append(AutoFix(
                id=f"oracle_number_{len(fixes)}",
                category=FixCategory.SYNTAX,
                severity=FixSeverity.HIGH,
                title="Convert Oracle NUMBER to PostgreSQL numeric type",
                description="Oracle NUMBER type requires conversion for PostgreSQL",
                original_code=match.group(0),
                fixed_code=fixed,
                explanation="Converts Oracle NUMBER to appropriate PostgreSQL numeric type",
                confidence_score=0.85,
                estimated_impact="Essential for data type compatibility",
                auto_apply=False,
                warnings=["Verify numeric precision requirements"]
            ))
        
        # VARCHAR2 to VARCHAR conversion
        varchar2_pattern = re.compile(r'\bVARCHAR2\b', re.IGNORECASE)
        if varchar2_pattern.search(schema_ddl):
            fixes.append(AutoFix(
                id=f"oracle_varchar2_{len(fixes)}",
                category=FixCategory.SYNTAX,
                severity=FixSeverity.MEDIUM,
                title="Convert VARCHAR2 to VARCHAR",
                description="PostgreSQL uses VARCHAR instead of VARCHAR2",
                original_code="VARCHAR2",
                fixed_code="VARCHAR",
                explanation="Direct replacement, PostgreSQL VARCHAR has same functionality",
                confidence_score=0.98,
                estimated_impact="Syntax compatibility improvement",
                auto_apply=True
            ))
        
        # SYSDATE to CURRENT_TIMESTAMP
        sysdate_pattern = re.compile(r'\bSYSDATE\b', re.IGNORECASE)
        if sysdate_pattern.search(schema_ddl):
            fixes.append(AutoFix(
                id=f"oracle_sysdate_{len(fixes)}",
                category=FixCategory.SYNTAX,
                severity=FixSeverity.MEDIUM,
                title="Convert SYSDATE to CURRENT_TIMESTAMP",
                description="PostgreSQL uses CURRENT_TIMESTAMP instead of SYSDATE",
                original_code="SYSDATE",
                fixed_code="CURRENT_TIMESTAMP",
                explanation="CURRENT_TIMESTAMP provides equivalent functionality",
                confidence_score=0.95,
                estimated_impact="Function compatibility improvement",
                auto_apply=True
            ))
        
        return fixes
    
    def _sqlserver_to_postgresql_schema_fixes(self, schema_ddl: str) -> List[AutoFix]:
        """SQL Server to PostgreSQL specific schema fixes"""
        fixes = []
        
        # IDENTITY to SERIAL conversion
        identity_pattern = re.compile(r'(\w+)\s+INT\s+IDENTITY(?:\(\d+,\s*\d+\))?', re.IGNORECASE)
        for match in identity_pattern.finditer(schema_ddl):
            original = match.group(0)
            column_name = match.group(1)
            fixed = f"{column_name} SERIAL"
            
            fixes.append(AutoFix(
                id=f"sqlserver_identity_{len(fixes)}",
                category=FixCategory.SYNTAX,
                severity=FixSeverity.HIGH,
                title="Convert IDENTITY to SERIAL",
                description="SQL Server IDENTITY converted to PostgreSQL SERIAL",
                original_code=original,
                fixed_code=fixed,
                explanation="PostgreSQL uses SERIAL for auto-incrementing columns",
                confidence_score=0.95,
                estimated_impact="Essential for PostgreSQL compatibility",
                auto_apply=True
            ))
        
        # NVARCHAR to VARCHAR conversion
        nvarchar_pattern = re.compile(r'\bNVARCHAR\b', re.IGNORECASE)
        if nvarchar_pattern.search(schema_ddl):
            fixes.append(AutoFix(
                id=f"sqlserver_nvarchar_{len(fixes)}",
                category=FixCategory.SYNTAX,
                severity=FixSeverity.MEDIUM,
                title="Convert NVARCHAR to VARCHAR",
                description="PostgreSQL uses VARCHAR for Unicode strings",
                original_code="NVARCHAR",
                fixed_code="VARCHAR",
                explanation="PostgreSQL VARCHAR natively supports Unicode",
                confidence_score=0.92,
                estimated_impact="Syntax compatibility improvement",
                auto_apply=True
            ))
        
        # GETDATE() to CURRENT_TIMESTAMP
        getdate_pattern = re.compile(r'\bGETDATE\(\)', re.IGNORECASE)
        if getdate_pattern.search(schema_ddl):
            fixes.append(AutoFix(
                id=f"sqlserver_getdate_{len(fixes)}",
                category=FixCategory.SYNTAX,
                severity=FixSeverity.MEDIUM,
                title="Convert GETDATE() to CURRENT_TIMESTAMP",
                description="PostgreSQL uses CURRENT_TIMESTAMP instead of GETDATE()",
                original_code="GETDATE()",
                fixed_code="CURRENT_TIMESTAMP",
                explanation="CURRENT_TIMESTAMP provides equivalent functionality",
                confidence_score=0.98,
                estimated_impact="Function compatibility improvement",
                auto_apply=True
            ))
        
        # BIT to BOOLEAN conversion
        bit_pattern = re.compile(r'(\w+)\s+BIT\b', re.IGNORECASE)
        for match in bit_pattern.finditer(schema_ddl):
            column_name = match.group(1)
            original = match.group(0)
            fixed = f"{column_name} BOOLEAN"
            
            fixes.append(AutoFix(
                id=f"sqlserver_bit_{len(fixes)}",
                category=FixCategory.SYNTAX,
                severity=FixSeverity.MEDIUM,
                title="Convert BIT to BOOLEAN",
                description="SQL Server BIT type converted to PostgreSQL BOOLEAN",
                original_code=original,
                fixed_code=fixed,
                explanation="PostgreSQL BOOLEAN is the equivalent of SQL Server BIT",
                confidence_score=0.90,
                estimated_impact="Data type compatibility improvement",
                auto_apply=True
            ))
        
        return fixes
    
    def _analyze_query_fixes(self, source_engine: str, target_engine: str, queries: str) -> List[AutoFix]:
        """Analyze and generate query compatibility fixes"""
        fixes = []
        
        # Split queries and analyze each
        query_list = [q.strip() for q in queries.split(';') if q.strip()]
        
        for i, query in enumerate(query_list):
            # Limit clause fixes for SQL Server
            if source_engine == 'sql_server' and 'postgresql' in target_engine:
                top_pattern = re.compile(r'SELECT\s+TOP\s+(\d+)', re.IGNORECASE)
                match = top_pattern.search(query)
                if match:
                    limit_num = match.group(1)
                    original_select = match.group(0)
                    fixed_query = query.replace(original_select, 'SELECT') + f' LIMIT {limit_num}'
                    
                    fixes.append(AutoFix(
                        id=f"query_top_limit_{i}",
                        category=FixCategory.COMPATIBILITY,
                        severity=FixSeverity.HIGH,
                        title="Convert TOP to LIMIT clause",
                        description="SQL Server TOP converted to PostgreSQL LIMIT",
                        original_code=query,
                        fixed_code=fixed_query,
                        explanation="PostgreSQL uses LIMIT instead of TOP for row limiting",
                        confidence_score=0.92,
                        estimated_impact="Query syntax compatibility",
                        auto_apply=True
                    ))
            
            # Date function conversions for MySQL
            if source_engine == 'mysql' and 'postgresql' in target_engine:
                date_format_pattern = re.compile(r'DATE_FORMAT\s*\(\s*([^,]+),\s*([^)]+)\)', re.IGNORECASE)
                match = date_format_pattern.search(query)
                if match:
                    date_expr = match.group(1)
                    format_expr = match.group(2)
                    
                    fixed_query = query.replace(
                        match.group(0),
                        f"TO_CHAR({date_expr}, {format_expr})"
                    )
                    
                    fixes.append(AutoFix(
                        id=f"mysql_date_format_{i}",
                        category=FixCategory.COMPATIBILITY,
                        severity=FixSeverity.MEDIUM,
                        title="Convert DATE_FORMAT to TO_CHAR",
                        description="MySQL DATE_FORMAT converted to PostgreSQL TO_CHAR",
                        original_code=query,
                        fixed_code=fixed_query,
                        explanation="PostgreSQL uses TO_CHAR for date formatting instead of DATE_FORMAT",
                        confidence_score=0.88,
                        estimated_impact="Query compatibility improvement",
                        auto_apply=False,
                        warnings=["Verify date format strings are compatible"]
                    ))
        
        return fixes
    
    def _analyze_performance_fixes(self, source_engine: str, target_engine: str, 
                                 schema_ddl: str, queries: str) -> List[AutoFix]:
        """Generate performance optimization fixes"""
        fixes = []
        
        # Index optimization for AWS
        if 'aurora' in target_engine or 'rds' in target_engine:
            # Look for tables that might benefit from composite indexes
            table_pattern = re.compile(r'CREATE TABLE (\w+)\s*\((.*?)\);', re.IGNORECASE | re.DOTALL)
            for match in table_pattern.finditer(schema_ddl):
                table_name = match.group(1)
                table_def = match.group(2)
                
                # Check for foreign key columns that could benefit from composite indexes
                if 'user_id' in table_def.lower() and 'created_at' in table_def.lower():
                    fixes.append(AutoFix(
                        id=f"aws_composite_index_{table_name}",
                        category=FixCategory.PERFORMANCE,
                        severity=FixSeverity.MEDIUM,
                        title=f"Add composite index for {table_name}",
                        description=f"Create composite index on user_id and created_at for better query performance",
                        original_code=f"-- No composite index on {table_name}",
                        fixed_code=f"CREATE INDEX idx_{table_name}_user_created ON {table_name}(user_id, created_at DESC);",
                        explanation="Composite indexes improve performance for queries filtering by user and ordering by date",
                        confidence_score=0.75,
                        estimated_impact="15-30% improvement for user-based queries",
                        auto_apply=False
                    ))
        
        # Query optimization
        if queries:
            # Check for SELECT * patterns
            select_all_pattern = re.compile(r'SELECT\s+\*\s+FROM', re.IGNORECASE)
            if select_all_pattern.search(queries):
                fixes.append(AutoFix(
                    id="perf_select_star",
                    category=FixCategory.PERFORMANCE,
                    severity=FixSeverity.LOW,
                    title="Optimize SELECT * queries",
                    description="SELECT * can impact performance, consider specifying columns",
                    original_code="SELECT * FROM table_name",
                    fixed_code="SELECT column1, column2, column3 FROM table_name",
                    explanation="Selecting specific columns reduces network traffic and improves performance",
                    confidence_score=0.70,
                    estimated_impact="10-20% performance improvement for large tables",
                    auto_apply=False,
                    warnings=["Requires application code review to identify needed columns"]
                ))
        
        return fixes
    
    def _analyze_security_fixes(self, source_engine: str, target_engine: str, schema_ddl: str) -> List[AutoFix]:
        """Generate security enhancement fixes"""
        fixes = []
        
        # Add encryption recommendations
        if 'password' in schema_ddl.lower() or 'pwd' in schema_ddl.lower():
            fixes.append(AutoFix(
                id="security_password_encryption",
                category=FixCategory.SECURITY,
                severity=FixSeverity.CRITICAL,
                title="Implement password field encryption",
                description="Detected password fields without explicit encryption",
                original_code="password VARCHAR(255)",
                fixed_code="password_hash VARCHAR(255) -- Use bcrypt or similar hashing",
                explanation="Password fields should be hashed, not stored in plain text",
                confidence_score=0.98,
                estimated_impact="Critical security improvement",
                auto_apply=False,
                warnings=["Requires application code changes for password hashing"]
            ))
        
        # Add audit trail recommendations
        if not re.search(r'created_at|updated_at|audit', schema_ddl, re.IGNORECASE):
            fixes.append(AutoFix(
                id="security_audit_fields",
                category=FixCategory.SECURITY,
                severity=FixSeverity.MEDIUM,
                title="Add audit trail fields",
                description="Tables lack audit trail capabilities",
                original_code="-- Add to each table:",
                fixed_code="""created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
created_by VARCHAR(100),
updated_by VARCHAR(100)""",
                explanation="Audit fields enable tracking of data changes",
                confidence_score=0.85,
                estimated_impact="Improved security and compliance",
                auto_apply=False
            ))
        
        # Row Level Security for PostgreSQL
        if 'postgresql' in target_engine:
            fixes.append(AutoFix(
                id="security_rls_postgresql",
                category=FixCategory.SECURITY,
                severity=FixSeverity.MEDIUM,
                title="Enable Row Level Security",
                description="Consider implementing Row Level Security for multi-tenant data",
                original_code="CREATE TABLE users (...);",
                fixed_code="""CREATE TABLE users (...);
ALTER TABLE users ENABLE ROW LEVEL SECURITY;
CREATE POLICY user_policy ON users FOR ALL TO application_role USING (user_id = current_setting('app.current_user_id')::INTEGER);""",
                explanation="RLS provides fine-grained access control at the row level",
                confidence_score=0.75,
                estimated_impact="Enhanced data isolation and security",
                auto_apply=False,
                prerequisites=["Multi-tenant application design", "User context management"]
            ))
        
        return fixes
    
    def _analyze_compliance_fixes(self, schema_ddl: str) -> List[AutoFix]:
        """Generate compliance-related fixes"""
        fixes = []
        
        # GDPR compliance for PII fields
        pii_patterns = ['email', 'phone', 'address', 'first_name', 'last_name']
        for pattern in pii_patterns:
            if re.search(pattern, schema_ddl, re.IGNORECASE):
                fixes.append(AutoFix(
                    id=f"compliance_gdpr_{pattern}",
                    category=FixCategory.COMPLIANCE,
                    severity=FixSeverity.HIGH,
                    title=f"GDPR compliance for {pattern} field",
                    description=f"Add GDPR compliance features for {pattern} data",
                    original_code=f"{pattern} VARCHAR(255)",
                    fixed_code=f"""{pattern} VARCHAR(255),
{pattern}_consent BOOLEAN DEFAULT FALSE,
{pattern}_consent_date TIMESTAMP,
data_retention_until TIMESTAMP""",
                    explanation="GDPR requires explicit consent tracking and data retention management",
                    confidence_score=0.88,
                    estimated_impact="GDPR compliance improvement",
                    auto_apply=False,
                    prerequisites=["Legal review", "Privacy policy updates"]
                ))
                break  # Only add once per schema
        
        return fixes
    
    def _aws_optimization_schema_fixes(self, schema_ddl: str, target_engine: str) -> List[AutoFix]:
        """Generate AWS-specific optimization fixes"""
        fixes = []
        
        # Aurora-specific optimizations
        if 'aurora' in target_engine:
            fixes.append(AutoFix(
                id="aws_aurora_optimization",
                category=FixCategory.OPTIMIZATION,
                severity=FixSeverity.LOW,
                title="Enable Aurora-specific optimizations",
                description="Configure Aurora-specific features for better performance",
                original_code="-- Standard configuration",
                fixed_code="""-- Aurora optimizations
-- Enable Aurora parallel query for analytics workloads
-- Configure Aurora auto scaling
-- Set up Aurora Global Database for multi-region access
-- Enable Aurora Serverless for variable workloads""",
                explanation="Aurora provides specific optimizations not available in standard RDS",
                confidence_score=0.80,
                estimated_impact="Potential 2-3x performance improvement for compatible workloads",
                auto_apply=False,
                prerequisites=["Aurora-compatible workload analysis"]
            ))
        
        return fixes
    
    async def _get_ai_enhanced_fixes(self, source_engine: str, target_engine: str, 
                                   schema_ddl: str, queries: str) -> List[AutoFix]:
        """Get AI-enhanced fix recommendations"""
        if not self.connected:
            return []
        
        try:
            # Simulate AI analysis for complex fixes
            await asyncio.sleep(0.5)
            
            fixes = []
            
            # AI-suggested performance optimization
            if schema_ddl:
                fixes.append(AutoFix(
                    id="ai_performance_optimization",
                    category=FixCategory.OPTIMIZATION,
                    severity=FixSeverity.MEDIUM,
                    title="AI-Suggested Index Optimization",
                    description="AI analysis suggests composite index optimization",
                    original_code="CREATE INDEX idx_user_email ON users(email);",
                    fixed_code="""CREATE INDEX idx_user_email_active ON users(email, is_active) WHERE is_active = true;
-- Partial index for better performance on active users""",
                    explanation="AI analysis of query patterns suggests this composite partial index",
                    confidence_score=0.82,
                    estimated_impact="20-30% query performance improvement",
                    auto_apply=False,
                    warnings=["Test performance impact in staging environment"]
                ))
            
            # AI-suggested schema normalization
            if 'address' in schema_ddl.lower():
                fixes.append(AutoFix(
                    id="ai_schema_normalization",
                    category=FixCategory.OPTIMIZATION,
                    severity=FixSeverity.LOW,
                    title="AI-Suggested Schema Normalization",
                    description="AI suggests normalizing address data into separate table",
                    original_code="address VARCHAR(500)",
                    fixed_code="""-- Create separate address table
CREATE TABLE addresses (
    id SERIAL PRIMARY KEY,
    street VARCHAR(255),
    city VARCHAR(100),
    state VARCHAR(50),
    zip_code VARCHAR(20),
    country VARCHAR(100)
);
-- Reference in main table
address_id INTEGER REFERENCES addresses(id)""",
                    explanation="Normalization can improve data consistency and reduce storage",
                    confidence_score=0.65,
                    estimated_impact="Improved data consistency, potential storage reduction",
                    auto_apply=False,
                    prerequisites=["Application refactoring", "Data migration planning"]
                ))
            
            return fixes
            
        except Exception as e:
            logger.error(f"AI-enhanced fixes failed: {e}")
            return []
    
    def _auto_apply_safe_fixes(self, fixes: List[AutoFix]) -> List[AutoFix]:
        """Auto-apply fixes that are marked as safe"""
        for fix in fixes:
            if fix.auto_apply and fix.confidence_score > 0.9 and fix.severity in [FixSeverity.LOW, FixSeverity.MEDIUM]:
                fix.status = FixStatus.APPLIED
        
        return fixes
    
    def apply_fixes(self, fixes: List[AutoFix], original_schema: str, original_queries: str = "") -> Dict[str, str]:
        """Apply selected fixes to the original code"""
        fixed_schema = original_schema
        fixed_queries = original_queries
        
        applied_fixes = [fix for fix in fixes if fix.status == FixStatus.APPLIED]
        
        for fix in applied_fixes:
            if fix.category in [FixCategory.SYNTAX, FixCategory.COMPATIBILITY, FixCategory.SECURITY]:
                # Apply schema fixes
                if fix.original_code in fixed_schema:
                    fixed_schema = fixed_schema.replace(fix.original_code, fix.fixed_code)
            elif fix.category == FixCategory.OPTIMIZATION:
                # Add optimization code
                fixed_schema += f"\n\n-- {fix.title}\n{fix.fixed_code}"
        
        return {
            'fixed_schema': fixed_schema,
            'fixed_queries': fixed_queries,
            'applied_fixes': len(applied_fixes)
        }
    
    def _generate_fix_result(self, fixes: List[AutoFix], original_schema: str, original_queries: str) -> AutoFixResult:
        """Generate comprehensive fix result"""
        
        total_issues = len(fixes)
        critical_issues = len([f for f in fixes if f.severity == FixSeverity.CRITICAL])
        applied_fixes = len([f for f in fixes if f.status == FixStatus.APPLIED])
        
        # Calculate compatibility scores
        compatibility_before = max(0, 100 - (total_issues * 10))
        compatibility_after = min(100, compatibility_before + (applied_fixes * 15))
        
        # Generate performance gains estimate
        perf_fixes = [f for f in fixes if f.category == FixCategory.PERFORMANCE]
        performance_gains = f"Estimated {len(perf_fixes) * 15}% performance improvement" if perf_fixes else "No performance issues detected"
        
        # Security improvements
        security_fixes = [f for f in fixes if f.category == FixCategory.SECURITY]
        security_improvements = [f.title for f in security_fixes]
        
        # Generate summary report
        summary_report = f"""
Auto-Fix Analysis Summary:
- Total Issues Detected: {total_issues}
- Critical Issues: {critical_issues}
- Fixes Available: {len(fixes)}
- Auto-Applied Fixes: {applied_fixes}
- Compatibility Score: {compatibility_before}% ‚Üí {compatibility_after}%
- Performance Impact: {performance_gains}
- Security Enhancements: {len(security_improvements)} recommendations
"""
        
        return AutoFixResult(
            total_issues=total_issues,
            fixes_available=len(fixes),
            fixes_applied=applied_fixes,
            critical_issues=critical_issues,
            performance_gains=performance_gains,
            security_improvements=security_improvements,
            compatibility_score_before=compatibility_before,
            compatibility_score_after=compatibility_after,
            fixes=fixes,
            summary_report=summary_report
        )
    
    def _get_fallback_fix_result(self) -> AutoFixResult:
        """Fallback result when auto-fix fails"""
        return AutoFixResult(
            total_issues=0,
            fixes_available=0,
            fixes_applied=0,
            critical_issues=0,
            performance_gains="Auto-fix analysis not available",
            security_improvements=[],
            compatibility_score_before=50.0,
            compatibility_score_after=50.0,
            fixes=[],
            summary_report="Auto-fix analysis failed. Manual review required."
        )
    
    def _load_fix_patterns(self) -> Dict:
        """Load fix patterns for different database engines"""
        return {
            'mysql_to_postgresql': {
                'AUTO_INCREMENT': 'SERIAL',
                'TINYINT': 'SMALLINT',
                'LONGTEXT': 'TEXT',
                'ENUM': 'CHECK constraint',
                'ENGINE=': ''
            },
            'oracle_to_postgresql': {
                'NUMBER': 'NUMERIC/INTEGER',
                'VARCHAR2': 'VARCHAR',
                'SYSDATE': 'CURRENT_TIMESTAMP',
                'DUAL': '',
                'ROWNUM': 'LIMIT'
            },
            'sqlserver_to_postgresql': {
                'IDENTITY': 'SERIAL',
                'NVARCHAR': 'VARCHAR',
                'GETDATE()': 'CURRENT_TIMESTAMP',
                'TOP': 'LIMIT',
                'ISNULL': 'COALESCE'
            }
        }

# Helper functions
def get_database_info(engine: str) -> Dict:
    """Get database-specific configuration"""
    return DATABASE_CONFIG.get(engine, {
        'display_name': engine.title(),
        'icon': 'üóÑÔ∏è',
        'schema_label': f'{engine.title()} Schema Definition',
        'query_label': f'{engine.title()} Queries',
        'schema_term': 'Schema Definition',
        'query_term': 'Queries',
        'file_extensions': ['.sql'],
        'aws_target_options': ['aurora_postgresql'],
        'enterprise_features': ['Standard Features'],
        'sample_schema': 'CREATE TABLE example (id INT PRIMARY KEY);',
        'sample_queries': 'SELECT * FROM example;'
    })

def render_enterprise_header():
    """Render enhanced enterprise header"""
    st.markdown("""
    <div class="main-header">
        <h1>üóÑÔ∏è Enterprise Database Migration Analyzer</h1>
        <div class="enterprise-badge">
            ‚ú® Enterprise Edition - AI-Powered Migration Intelligence
        </div>
        <p style="font-size: 1.2rem; margin-top: 1rem;">
            Advanced Database Migration Analysis ‚Ä¢ Real-time AWS Cost Optimization ‚Ä¢ AI-Powered Risk Assessment ‚Ä¢ Enterprise Collaboration
        </p>
        <p style="font-size: 0.9rem; margin-top: 0.5rem; opacity: 0.9;">
            ü§ñ AI Analysis ‚Ä¢ üí∞ Cost Optimization ‚Ä¢ üîí Security Assessment ‚Ä¢ üë• Team Collaboration ‚Ä¢ üìä Performance Prediction ‚Ä¢ ‚òÅÔ∏è Multi-Cloud Support
        </p>
    </div>
    """, unsafe_allow_html=True)

def render_enhanced_sidebar():
    """Render enhanced sidebar with enterprise features"""
    st.sidebar.header("üîß Enterprise Migration Configuration")
    
    # User Management Section
    with st.sidebar.expander("üë§ User & Project Management", expanded=False):
        st.write("**Current User:**")
        st.write(f"User ID: {st.session_state.user_id[:8]}...")
        
        # Project selection
        db_manager = EnterpriseDBManager()
        projects = db_manager.get_user_projects(st.session_state.user_id)
        
        if projects:
            project_options = {p['name']: p['id'] for p in projects}
            selected_project = st.selectbox("Select Project", options=list(project_options.keys()), key="sidebar_project_select")
            if selected_project:
                st.session_state.current_project = project_options[selected_project]
        
        # Create new project
        if st.button("‚ûï Create New Project", key="sidebar_create_project"):
            st.session_state.show_project_creator = True
    
    # Enhanced Database Configuration
    st.sidebar.subheader("üì§ Source Database")
    
    # Check if example was loaded
    example_source = st.session_state.get('example_source', 'mysql')
    
    source_engine = st.sidebar.selectbox(
        "Source Database Engine",
        ["mysql", "postgresql", "oracle", "sql_server", "mongodb"],
        index=["mysql", "postgresql", "oracle", "sql_server", "mongodb"].index(example_source) if example_source in ["mysql", "postgresql", "oracle", "sql_server", "mongodb"] else 0,
        format_func=lambda x: f"{DATABASE_CONFIG[x]['icon']} {DATABASE_CONFIG[x]['display_name']}",
        key="sidebar_source_engine"
    )
    
    source_info = DATABASE_CONFIG[source_engine]
    source_version = st.sidebar.text_input("Source Version", "Latest", key="sidebar_source_version")
    
    # Enhanced source info display
    st.sidebar.markdown(f"""
    <div class="sidebar-info">
        <strong>{source_info['icon']} {source_info['display_name']}</strong><br>
        Schema: {source_info['schema_term']}<br>
        Queries: {source_info['query_term']}<br>
        <strong>Enterprise Features:</strong><br>
        {'<br>'.join(['‚Ä¢ ' + feature for feature in source_info.get('enterprise_features', [])])}
    </div>
    """, unsafe_allow_html=True)
    
    # Target Database Configuration
    st.sidebar.subheader("üì• Target Database (AWS)")
    
    # Enhanced target options based on source
    if source_engine in DATABASE_CONFIG:
        target_options = DATABASE_CONFIG[source_engine].get('aws_target_options', ['aurora_postgresql', 'aurora_mysql'])
        # Add common AWS options
        all_aws_options = ['aurora_postgresql', 'aurora_mysql', 'rds_postgresql', 'rds_mysql', 'rds_oracle', 'rds_sqlserver', 'documentdb']
        target_options.extend([opt for opt in all_aws_options if opt not in target_options])
        target_options = target_options[:6]  # Limit options
    else:
        target_options = ['aurora_postgresql', 'aurora_mysql', 'rds_postgresql', 'rds_mysql']
    
    example_target = st.session_state.get('example_target', target_options[0])
    
    target_engine = st.sidebar.selectbox(
        "Target AWS Service",
        target_options,
        index=target_options.index(example_target) if example_target in target_options else 0,
        format_func=lambda x: {
            'aurora_mysql': 'üåü Aurora MySQL',
            'aurora_postgresql': 'üåü Aurora PostgreSQL',
            'rds_mysql': 'üóÑÔ∏è RDS MySQL',
            'rds_postgresql': 'üóÑÔ∏è RDS PostgreSQL',
            'rds_oracle': 'üóÑÔ∏è RDS Oracle',
            'rds_sqlserver': 'üóÑÔ∏è RDS SQL Server',
            'documentdb': 'üçÉ DocumentDB'
        }.get(x, x.title()),
        key="sidebar_target_engine"
    )
    
    # Enhanced AWS Configuration
    with st.sidebar.expander("‚öôÔ∏è AWS Configuration", expanded=True):
        instance_class = st.selectbox(
            "Instance Class",
            ["db.t3.micro", "db.t3.small", "db.t3.medium", "db.t3.large", 
             "db.m5.large", "db.m5.xlarge", "db.r5.large", "db.r5.xlarge"],
            key="sidebar_instance_class"
        )
        
        storage_gb = st.number_input("Storage (GB)", min_value=20, max_value=10000, value=100, key="sidebar_storage_gb")
        storage_type = st.selectbox("Storage Type", ["gp2", "gp3", "io1", "io2"], index=1, key="sidebar_storage_type")
        multi_az = st.checkbox("Multi-AZ Deployment", value=True, key="sidebar_multi_az")
        backup_retention = st.slider("Backup Retention (days)", 1, 35, 7, key="sidebar_backup_retention")
    
    # Enhanced Security Configuration
    with st.sidebar.expander("üîí Security Configuration", expanded=False):
        encryption_at_rest = st.checkbox("Encryption at Rest", value=True, key="sidebar_encryption_rest")
        encryption_in_transit = st.checkbox("Encryption in Transit", value=True, key="sidebar_encryption_transit")
        iam_auth = st.checkbox("IAM Database Authentication", value=False, key="sidebar_iam_auth")
        
        compliance_requirements = st.multiselect(
            "Compliance Requirements",
            ["GDPR", "HIPAA", "SOC2", "PCI_DSS"],
            default=["GDPR"],
            key="sidebar_compliance"
        )
    
    # Enhanced Migration Scope
    st.sidebar.subheader("üéØ Migration Scope")
    
    with st.sidebar.expander("üìã Migration Options", expanded=True):
        include_schema = st.checkbox("Include Schema Objects", True, key="sidebar_include_schema")
        include_data = st.checkbox("Include Data Migration", True, key="sidebar_include_data")
        include_procedures = st.checkbox("Include Stored Procedures", True, key="sidebar_include_procedures")
        include_triggers = st.checkbox("Include Triggers", False, key="sidebar_include_triggers")
        
        # Business context
        business_critical = st.checkbox("Business Critical System", False, key="sidebar_business_critical")
        downtime_tolerance = st.selectbox(
            "Downtime Tolerance",
            ["< 1 hour", "< 4 hours", "< 24 hours", "Flexible"],
            key="sidebar_downtime_tolerance"
        )
    
    # Enhanced Analysis Options
    st.sidebar.subheader("üîç Analysis Options")
    
    with st.sidebar.expander("ü§ñ AI Analysis", expanded=True):
        enable_ai_analysis = st.checkbox("Enable AI Analysis", True, key="sidebar_enable_ai")
        analysis_depth = st.selectbox("Analysis Depth", ["Standard", "Comprehensive", "Expert"], key="sidebar_analysis_depth")
        
        if enable_ai_analysis:
            st.markdown('<span class="feature-badge badge-ai">ü§ñ AI Enhanced</span>', unsafe_allow_html=True)
    
    with st.sidebar.expander("üí∞ Cost Analysis", expanded=True):
        enable_cost_analysis = st.checkbox("Real-time Cost Analysis", True, key="sidebar_enable_cost")
        enable_optimization = st.checkbox("Cost Optimization", True, key="sidebar_enable_optimization")
        
        if enable_cost_analysis:
            st.markdown('<span class="feature-badge badge-new">üí∞ Live Pricing</span>', unsafe_allow_html=True)
    
    with st.sidebar.expander("üîí Security Analysis", expanded=True):
        enable_security_scan = st.checkbox("Security Assessment", True, key="sidebar_enable_security")
        enable_compliance_check = st.checkbox("Compliance Validation", True, key="sidebar_enable_compliance")
        
        if enable_security_scan:
            st.markdown('<span class="feature-badge badge-enterprise">üîí Enterprise</span>', unsafe_allow_html=True)
    
    # Advanced Options
    with st.sidebar.expander("‚öôÔ∏è Advanced Options", expanded=False):
        generate_scripts = st.checkbox("Generate Migration Scripts", True, key="sidebar_generate_scripts")
        enable_monitoring = st.checkbox("Setup Monitoring", True, key="sidebar_enable_monitoring")
        enable_collaboration = st.checkbox("Team Collaboration", False, key="sidebar_enable_collaboration")
        
        # Team settings
        if enable_collaboration:
            st.session_state.collaboration_enabled = True
            team_size = st.number_input("Team Size", min_value=1, max_value=20, value=3, key="sidebar_team_size")
            st.markdown('<span class="feature-badge badge-enterprise">üë• Collaboration</span>', unsafe_allow_html=True)
    
    return {
        'source_engine': source_engine,
        'source_version': source_version,
        'target_engine': target_engine,
        'instance_class': instance_class,
        'storage_gb': storage_gb,
        'storage_type': storage_type,
        'multi_az': multi_az,
        'backup_retention': backup_retention,
        'backup_retention_days': backup_retention,
        'encryption_at_rest': encryption_at_rest,
        'encryption_in_transit': encryption_in_transit,
        'iam_auth': iam_auth,
        'compliance_requirements': compliance_requirements,
        'include_schema': include_schema,
        'include_data': include_data,
        'include_procedures': include_procedures,
        'include_triggers': include_triggers,
        'business_critical': business_critical,
        'downtime_tolerance': downtime_tolerance,
        'enable_ai_analysis': enable_ai_analysis,
        'analysis_depth': analysis_depth,
        'enable_cost_analysis': enable_cost_analysis,
        'enable_optimization': enable_optimization,
        'enable_security_scan': enable_security_scan,
        'enable_compliance_check': enable_compliance_check,
        'generate_scripts': generate_scripts,
        'enable_monitoring': enable_monitoring,
        'enable_collaboration': enable_collaboration
    }

def render_enterprise_dashboard_tab():
    """Render enterprise dashboard with key metrics"""
    st.subheader("üìä Enterprise Migration Dashboard")
    
    # Quick stats
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric(
            "üöÄ Active Projects",
            len(st.session_state.get('projects', [])),
            delta="+2 this month"
        )
    
    with col2:
        st.metric(
            "üéØ Success Rate",
            "94.2%",
            delta="+2.1%"
        )
    
    with col3:
        st.metric(
            "üí∞ Cost Savings",
            "$2.4M",
            delta="+$450K"
        )
    
    with col4:
        st.metric(
            "‚ö° Avg Migration Time",
            "3.2 weeks",
            delta="-0.8 weeks"
        )
    
    # Recent activity
    st.markdown("**üìà Recent Activity:**")
    
    activity_data = pd.DataFrame({
        'Date': pd.date_range(start='2024-01-01', periods=30, freq='D'),
        'Migrations': np.random.poisson(3, 30),
        'Cost Savings': np.random.normal(50000, 15000, 30)
    })
    
    col1, col2 = st.columns(2)
    
    with col1:
        fig = px.line(activity_data, x='Date', y='Migrations', title='Daily Migrations')
        fig.update_layout(height=300)
        st.plotly_chart(fig, use_container_width=True, key="daily_migrations")
    
    with col2:
        fig = px.bar(activity_data.tail(7), x='Date', y='Cost Savings', title='Weekly Cost Savings')
        fig.update_layout(height=300)
        st.plotly_chart(fig, use_container_width=True, key="weekly_cost_savings")
    
    # Project status overview
    st.markdown("**üìã Project Status Overview:**")
    
    project_status = pd.DataFrame({
        'Status': ['Planning', 'In Progress', 'Testing', 'Completed', 'On Hold'],
        'Count': [5, 8, 3, 12, 2],
        'Color': ['#3b82f6', '#f59e0b', '#8b5cf6', '#22c55e', '#ef4444']
    })
    
    fig = px.pie(project_status, values='Count', names='Status', 
                 color_discrete_sequence=project_status['Color'],
                 title='Migration Projects by Status')
    fig.update_layout(height=400)
    st.plotly_chart(fig, use_container_width=True, key="migration_projects_by_status")

def render_examples_tab():
    """Render examples tab with enhanced tutorials"""
    st.subheader("üìö Migration Examples & Tutorials")
    st.markdown('<span class="feature-badge badge-enhanced">üìö Enhanced Tutorials</span>', unsafe_allow_html=True)
    
    # Quick start guide with enterprise features
    st.markdown("""
    <div class="analysis-card">
        <h4>üöÄ Enterprise Quick Start Guide</h4>
        <p><strong>Step 1:</strong> Configure source and target databases in the enterprise sidebar</p>
        <p><strong>Step 2:</strong> Enable enterprise features (AI Analysis, Cost Optimization, Security Scanning)</p>
        <p><strong>Step 3:</strong> Create or select a project for collaboration</p>
        <p><strong>Step 4:</strong> Run comprehensive analysis across all enterprise modules</p>
        <p><strong>Step 5:</strong> Review AI recommendations and cost optimizations</p>
        <p><strong>Step 6:</strong> Generate migration scripts and security reports</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Example scenarios
    st.markdown("**‚ú® Quick Start Examples:**")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("üê¨ MySQL ‚Üí Aurora PostgreSQL", key="example_mysql_aurora"):
            st.session_state.example_source = 'mysql'
            st.session_state.example_target = 'aurora_postgresql'
            st.session_state.example_schema = DATABASE_CONFIG['mysql']['sample_schema']
            st.success("‚úÖ MySQL to Aurora PostgreSQL example loaded!")
            st.rerun()
    
    with col2:
        if st.button("üêò PostgreSQL ‚Üí Aurora PostgreSQL", key="example_postgres_aurora"):
            st.session_state.example_source = 'postgresql'
            st.session_state.example_target = 'aurora_postgresql'
            st.session_state.example_schema = DATABASE_CONFIG['postgresql']['sample_schema']
            st.success("‚úÖ PostgreSQL to Aurora PostgreSQL example loaded!")
            st.rerun()
    
    with col3:
        if st.button("üî¥ Oracle ‚Üí RDS PostgreSQL", key="example_oracle_rds"):
            st.session_state.example_source = 'oracle'
            st.session_state.example_target = 'rds_postgresql'
            st.session_state.example_schema = DATABASE_CONFIG['oracle']['sample_schema']
            st.success("‚úÖ Oracle to RDS PostgreSQL example loaded!")
            st.rerun()
    
    # Feature showcase
    st.markdown("**üåü Enterprise Features Showcase:**")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("""
        <div class="enterprise-card">
            <h4>ü§ñ AI-Powered Analysis</h4>
            <p>‚Ä¢ Comprehensive migration strategy</p>
            <p>‚Ä¢ Risk assessment & mitigation</p>
            <p>‚Ä¢ Performance prediction</p>
            <p>‚Ä¢ Timeline estimation</p>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown("""
        <div class="cost-card">
            <h4>üí∞ Cost Optimization</h4>
            <p>‚Ä¢ Real-time AWS pricing</p>
            <p>‚Ä¢ Reserved instance recommendations</p>
            <p>‚Ä¢ Storage optimization</p>
            <p>‚Ä¢ ROI analysis</p>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        st.markdown("""
        <div class="security-card">
            <h4>üîí Security & Compliance</h4>
            <p>‚Ä¢ Data classification</p>
            <p>‚Ä¢ Compliance validation</p>
            <p>‚Ä¢ Vulnerability assessment</p>
            <p>‚Ä¢ Security recommendations</p>
        </div>
        """, unsafe_allow_html=True)

def render_schema_input_tab(config: Dict):
    """Enhanced schema input tab"""
    st.subheader("üìã Enhanced Schema Analysis Input")
    
    # Get database-specific info
    source_info = get_database_info(config['source_engine'])
    target_info = get_database_info(config['target_engine'].replace('aurora_', '').replace('rds_', '').replace('documentdb', 'mongodb'))
    
    # Enhanced migration direction display
    st.markdown(f"""
    <div class="enterprise-card">
        <h4>üîÑ Enterprise Migration Configuration</h4>
        <p><strong>Source:</strong> {source_info['icon']} {source_info['display_name']} ({config['source_version']})</p>
        <p><strong>Target:</strong> ‚òÅÔ∏è AWS {config['target_engine'].replace('_', ' ').title()}</p>
        <p><strong>Project:</strong> {st.session_state.get('current_project', 'Default Project')[:8] if st.session_state.get('current_project') else 'Default Project'}...</p>
        <div style="margin-top: 0.5rem;">
            <span class="feature-badge badge-enterprise">üè¢ Enterprise</span>
            {f'<span class="feature-badge badge-ai">ü§ñ AI Ready</span>' if config.get('enable_ai_analysis') else ''}
            {f'<span class="feature-badge badge-new">üí∞ Cost Analysis</span>' if config.get('enable_cost_analysis') else ''}
        </div>
    </div>
    """, unsafe_allow_html=True)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown(f"**üì• {source_info['schema_label']} Input:**")
        
        # Check if example schema is loaded
        example_schema = st.session_state.get('example_schema', '')
        if example_schema:
            st.info(f"üìö Example schema loaded from migration scenarios!")
        
        input_method = st.radio(
            "Choose input method:",
            ["Manual Entry", "File Upload"],
            help=f"Select how you want to provide {source_info['schema_term'].lower()} information",
            key="schema_input_method"
        )
        
        if input_method == "Manual Entry":
            default_schema = example_schema if example_schema else source_info['sample_schema']
            schema_ddl = st.text_area(
                f"{source_info['schema_label']}",
                value=default_schema,
                height=300,
                help=f"Enter your {source_info['schema_term'].lower()} definition here",
                key="schema_ddl_input"
            )
            
        elif input_method == "File Upload":
            uploaded_file = st.file_uploader(
                f"Upload {source_info['display_name']} Schema File",
                type=[ext.replace('.', '') for ext in source_info['file_extensions']],
                help=f"Upload a file containing your {source_info['schema_term'].lower()}",
                key="schema_file_upload"
            )
            
            if uploaded_file:
                schema_ddl = uploaded_file.read().decode('utf-8')
                st.text_area(f"Uploaded {source_info['schema_term']} Preview", 
                           schema_ddl[:1000] + "..." if len(schema_ddl) > 1000 else schema_ddl, 
                           height=200,
                           key="schema_preview")
            else:
                schema_ddl = example_schema if example_schema else ""
    
    with col2:
        st.markdown(f"**üìù {source_info['query_label']} Analysis:**")
        
        queries_text = st.text_area(
            f"{source_info['query_label']} to Analyze",
            placeholder=source_info.get('sample_queries', 'Enter your queries here...'),
            height=300,
            help=f"Enter {source_info['query_term'].lower()} that you want to analyze for compatibility",
            key="queries_input"
        )
        
        # Enhanced query analysis info
        st.markdown(f"""
        <div class="analysis-card">
            <h5>üîÑ Enhanced Query Analysis</h5>
            <p><strong>From:</strong> {source_info['query_term']} ({source_info['display_name']})</p>
            <p><strong>To:</strong> AWS Compatible SQL</p>
            <p><strong>Features:</strong></p>
            <p>‚Ä¢ Function mapping & conversion</p>
            <p>‚Ä¢ Performance impact analysis</p>
            <p>‚Ä¢ Syntax compatibility checking</p>
            <p>‚Ä¢ AI-powered optimization suggestions</p>
        </div>
        """, unsafe_allow_html=True)
    
    # Clear example data button
    if example_schema and st.button("üóëÔ∏è Clear Example Data", key="clear_example_data"):
        st.session_state.pop('example_schema', None)
        st.session_state.pop('example_source', None)
        st.session_state.pop('example_target', None)
        st.rerun()
    
    return schema_ddl, queries_text

def render_compatibility_analysis_tab(config: Dict, schema_ddl: str, queries_text: str):
    """Enhanced compatibility analysis with enterprise features"""
    st.subheader("üîç Enhanced Compatibility Analysis")
    st.markdown('<span class="feature-badge badge-enhanced">üîç Enhanced Analysis</span>', unsafe_allow_html=True)
    
    if not schema_ddl and not queries_text:
        st.markdown("""
        <div class="warning-banner">
            <h4>‚ö†Ô∏è Analysis Input Required</h4>
            <p>Please provide schema DDL or queries in the Schema Input tab to run comprehensive compatibility analysis.</p>
        </div>
        """, unsafe_allow_html=True)
        return
    
    # Get database info
    source_info = get_database_info(config['source_engine'])
    
    # Enhanced analysis header
    st.markdown(f"""
    <div class="enterprise-card">
        <h4>üìä {source_info['display_name']} ‚Üí AWS {config['target_engine'].replace('_', ' ').title()} Compatibility Analysis</h4>
        <p>Comprehensive migration compatibility assessment with enterprise-grade analysis</p>
    </div>
    """, unsafe_allow_html=True)
    
    if st.button("üöÄ Run Enhanced Compatibility Analysis", type="primary", key="run_compatibility_analysis"):
        with st.spinner("üîÑ Running comprehensive compatibility analysis..."):
            
            # Simulate enhanced analysis (in production, use actual SchemaAnalyzer)
            time.sleep(2)  # Simulate processing
            
            # Mock analysis results with enhanced features
            compatibility_score = 85.5
            complexity_level = "medium"
            issues_found = 3
            recommendations_count = 7
            
            # Display enhanced results
            st.markdown("**üìä Enhanced Analysis Results:**")
            
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                score_color = "success" if compatibility_score > 80 else "warning" if compatibility_score > 60 else "error"
                if score_color == "success":
                    st.success(f"üéØ Compatibility: {compatibility_score}%")
                elif score_color == "warning":
                    st.warning(f"üéØ Compatibility: {compatibility_score}%")
                else:
                    st.error(f"üéØ Compatibility: {compatibility_score}%")
            
            with col2:
                if issues_found <= 2:
                    st.success(f"‚ö†Ô∏è Issues: {issues_found}")
                elif issues_found <= 5:
                    st.warning(f"‚ö†Ô∏è Issues: {issues_found}")
                else:
                    st.error(f"‚ö†Ô∏è Issues: {issues_found}")
            
            with col3:
                st.info(f"üí° Recommendations: {recommendations_count}")
            
            with col4:
                migration_effort = "Medium" if compatibility_score > 70 else "High"
                effort_color = "warning" if migration_effort == "Medium" else "error"
                if effort_color == "warning":
                    st.warning(f"‚è±Ô∏è Effort: {migration_effort}")
                else:
                    st.error(f"‚è±Ô∏è Effort: {migration_effort}")
            
            # Enhanced detailed analysis
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("""
                <div class="analysis-card">
                    <h4>üîß Technical Analysis</h4>
                    <p><strong>Schema Conversion:</strong> 92% compatible</p>
                    <p><strong>Data Type Mapping:</strong> 88% direct mapping</p>
                    <p><strong>Index Conversion:</strong> 95% compatible</p>
                    <p><strong>Constraint Handling:</strong> 85% compatible</p>
                    <div class="progress-indicator">
                        <div class="progress-fill progress-high" style="width: 85%"></div>
                    </div>
                </div>
                """, unsafe_allow_html=True)
                
                st.markdown("""
                <div class="enterprise-card">
                    <h4>‚ö†Ô∏è Identified Issues</h4>
                    <p>‚Ä¢ AUTO_INCREMENT conversion needed</p>
                    <p>‚Ä¢ ENUM types require CHECK constraints</p>
                    <p>‚Ä¢ Full-text search syntax differences</p>
                </div>
                """, unsafe_allow_html=True)
            
            with col2:
                st.markdown("""
                <div class="cost-card">
                    <h4>üí° Enterprise Recommendations</h4>
                    <p>‚Ä¢ Use AWS DMS for migration</p>
                    <p>‚Ä¢ Implement connection pooling</p>
                    <p>‚Ä¢ Enable Performance Insights</p>
                    <p>‚Ä¢ Configure automated backups</p>
                    <p>‚Ä¢ Set up CloudWatch monitoring</p>
                </div>
                """, unsafe_allow_html=True)
                
                if config.get('enable_ai_analysis', False):
                    st.markdown("""
                    <div class="ai-enhanced-card">
                        <h4>ü§ñ AI Insights Available</h4>
                        <p>Run AI Analysis for comprehensive migration strategy, risk assessment, and optimization recommendations.</p>
                    </div>
                    """, unsafe_allow_html=True)
            
            # Performance prediction
            st.markdown("**üìà Performance Impact Prediction:**")
            
            performance_data = pd.DataFrame({
                'Metric': ['Query Performance', 'Connection Handling', 'Storage I/O', 'Memory Usage'],
                'Current': [100, 100, 100, 100],
                'Predicted': [105, 115, 98, 102],
                'Impact': ['Slight Improvement', 'Improvement', 'Minimal Impact', 'Minimal Impact']
            })
            
            fig = px.bar(performance_data, x='Metric', y=['Current', 'Predicted'],
                        title='Performance Impact Prediction',
                        barmode='group')
            fig.update_layout(height=400)
            st.plotly_chart(fig, use_container_width=True, key="performance_impact_prediction")

def render_enhanced_cost_analysis_tab(config: Dict):
    """Render enhanced cost analysis with optimization recommendations"""
    st.subheader("üí∞ Enhanced AWS Cost Analysis & Optimization")
    
    st.markdown('<span class="feature-badge badge-new">üí∞ Real-time AWS Pricing</span>', unsafe_allow_html=True)
    
    cost_calculator = EnhancedAWSCostCalculator()
    
    if not cost_calculator.connected:
        st.markdown("""
        <div class="warning-banner">
            <h4>‚ö†Ô∏è Using Estimated Pricing</h4>
            <p>AWS SDK not configured. Using estimated pricing for demonstration purposes.</p>
        </div>
        """, unsafe_allow_html=True)
    
    # Enhanced cost configuration
    st.markdown("**üéØ Cost Analysis Configuration:**")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("**üíª Compute Configuration:**")
        # Use config values from sidebar
        st.info(f"Instance: {config['instance_class']}")
        st.info(f"Multi-AZ: {config['multi_az']}")
        
        # Additional options
        reserved_instance = st.checkbox("Reserved Instance (1 year)", False, key="cost_reserved_instance")
        if reserved_instance:
            st.markdown('<span class="feature-badge badge-enterprise">üíæ 40% Savings</span>', unsafe_allow_html=True)
    
    with col2:
        st.markdown("**üíæ Storage Configuration:**")
        st.info(f"Storage: {config['storage_gb']} GB")
        st.info(f"Type: {config['storage_type']}")
        
        if config['storage_type'] == "gp3":
            st.markdown('<span class="feature-badge badge-new">‚ö° Optimized</span>', unsafe_allow_html=True)
    
    with col3:
        st.markdown("**üöö Migration Configuration:**")
        migration_duration = st.number_input("Migration Duration (hours)", min_value=1, max_value=168, value=24, key="cost_migration_duration")
        dms_instance = st.selectbox("DMS Instance", ["dms.t3.medium", "dms.t3.large", "dms.c5.xlarge"], key="cost_dms_instance")
        
        data_size_estimate = st.number_input("Data Size (GB)", min_value=1, max_value=100000, value=config['storage_gb'], key="cost_data_size")
    
    # Advanced cost factors
    with st.expander("üîß Advanced Cost Factors", expanded=False):
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**üìä Usage Patterns:**")
            cpu_utilization = st.slider("Expected CPU Utilization (%)", 0, 100, 70, key="cost_cpu_utilization")
            connection_count = st.number_input("Concurrent Connections", min_value=1, max_value=10000, value=100, key="cost_connection_count")
        
        with col2:
            st.markdown("**üåç Geographic Distribution:**")
            primary_region = st.selectbox("Primary Region", ["us-east-1", "us-west-2", "eu-west-1", "ap-southeast-1"], key="cost_primary_region")
            cross_region_backup = st.checkbox("Cross-Region Backup", True, key="cost_cross_region_backup")
    
    if st.button("üí∞ Calculate Comprehensive Cost Analysis", type="primary", key="calculate_cost_analysis"):
        with st.spinner("üîÑ Analyzing costs with real-time AWS pricing..."):
            
            # Enhanced config for cost calculation
            enhanced_config = {
                **config,
                'migration_duration_hours': migration_duration,
                'dms_instance': dms_instance,
                'data_size_gb': data_size_estimate,
                'reserved_instance': reserved_instance,
                'cpu_utilization': cpu_utilization,
                'connection_count': connection_count,
                'primary_region': primary_region,
                'cross_region_backup': cross_region_backup
            }
            
            # Calculate comprehensive cost estimate
            cost_estimate = cost_calculator.estimate_total_migration_cost(enhanced_config)
            
            # Store in session state
            st.session_state.cost_estimates[config['target_engine']] = cost_estimate
            
            # Display results
            st.markdown("**üí∞ Comprehensive Cost Analysis Results:**")
            
            # Key metrics
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                savings_percent = 40 if reserved_instance else 0
                monthly_display = cost_estimate.monthly_cost * (1 - savings_percent/100) if reserved_instance else cost_estimate.monthly_cost
                st.metric(
                    "üí∞ Monthly Cost",
                    f"${monthly_display:.2f}",
                    delta=f"-${cost_estimate.monthly_cost * savings_percent/100:.2f}" if reserved_instance else None
                )
            
            with col2:
                st.metric(
                    "üìÖ Annual Cost",
                    f"${cost_estimate.annual_cost:.2f}",
                    delta=f"Confidence: {cost_estimate.confidence_score:.0%}"
                )
            
            with col3:
                migration_cost = cost_estimate.cost_factors.get('migration', 0) + cost_estimate.cost_factors.get('data_transfer', 0)
                st.metric(
                    "üöö Migration Cost",
                    f"${migration_cost:.2f}",
                    delta="One-time"
                )
            
            with col4:
                total_first_year = cost_estimate.annual_cost + migration_cost
                if reserved_instance:
                    total_first_year *= 0.6  # Apply RI discount
                st.metric(
                    "üìä First Year Total",
                    f"${total_first_year:.2f}",
                    delta="Including migration"
                )
            
            # Detailed cost breakdown
            st.markdown("**üìä Detailed Cost Breakdown:**")
            
            cost_breakdown_data = pd.DataFrame({
                'Component': list(cost_estimate.cost_factors.keys()),
                'Monthly Cost': [v if k not in ['migration', 'data_transfer'] else 0 for k, v in cost_estimate.cost_factors.items()],
                'One-time Cost': [v if k in ['migration', 'data_transfer'] else 0 for k, v in cost_estimate.cost_factors.items()]
            })
            
            col1, col2 = st.columns(2)
            
            with col1:
                fig = px.pie(cost_breakdown_data, values='Monthly Cost', names='Component', 
                           title='Monthly Cost Distribution')
                fig.update_layout(height=400)
                st.plotly_chart(fig, use_container_width=True, key="monthly_cost_distribution")
            
            with col2:
                fig = px.bar(cost_breakdown_data, x='Component', y=['Monthly Cost', 'One-time Cost'],
                           title='Cost Breakdown by Component')
                fig.update_layout(height=400)
                st.plotly_chart(fig, use_container_width=True, key="cost_breakdown_component")
            
            # Cost optimization recommendations
            st.markdown("**üéØ Cost Optimization Recommendations:**")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown(f"""
                <div class="cost-optimization">
                    <h4>üí° Immediate Optimizations</h4>
                    {''.join([f'<p>‚Ä¢ {opt}</p>' for opt in cost_estimate.optimizations[:4]])}
                </div>
                """, unsafe_allow_html=True)
            
            with col2:
                # Calculate potential savings
                potential_savings = []
                if not reserved_instance:
                    potential_savings.append(f"Reserved Instances: ${cost_estimate.annual_cost * 0.4:.2f}/year")
                if config['storage_type'] == "gp2":
                    potential_savings.append(f"gp3 Storage: ${config['storage_gb'] * 0.035 * 12:.2f}/year")
                if not config['multi_az']:
                    potential_savings.append("Multi-AZ: High availability benefit")
                
                st.markdown(f"""
                <div class="enterprise-card">
                    <h4>üí∞ Potential Annual Savings</h4>
                    {''.join([f'<p>‚Ä¢ {saving}</p>' for saving in potential_savings[:4]])}
                </div>
                """, unsafe_allow_html=True)
            
            # ROI Analysis
            st.markdown("**üìà ROI Analysis:**")
            
            # Simulate current on-premises costs
            current_monthly_cost = cost_estimate.monthly_cost * 1.5  # Assume higher on-prem costs
            annual_savings = (current_monthly_cost - cost_estimate.monthly_cost) * 12
            payback_period = migration_cost / max(annual_savings, 1) * 12  # months
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric(
                    "üíµ Annual Savings",
                    f"${annual_savings:.2f}",
                    delta="vs Current Infrastructure"
                )
            
            with col2:
                st.metric(
                    "‚è±Ô∏è Payback Period",
                    f"{payback_period:.1f} months",
                    delta="Break-even point"
                )
            
            with col3:
                three_year_roi = (annual_savings * 3 - migration_cost) / migration_cost * 100
                st.metric(
                    "üìä 3-Year ROI",
                    f"{three_year_roi:.0f}%",
                    delta="Return on Investment"
                )

def render_enhanced_security_tab(config: Dict, schema_ddl: str):
    """Render enhanced security analysis"""
    st.subheader("üîí Enhanced Security & Compliance Analysis")
    
    st.markdown('<span class="feature-badge badge-enterprise">üîí Enterprise Security</span>', unsafe_allow_html=True)
    
    if not schema_ddl and not config.get('enable_security_scan', False):
        st.markdown("""
        <div class="warning-banner">
            <h4>‚ö†Ô∏è Security Analysis Requirements</h4>
            <p>Please provide schema DDL and enable security scanning to perform comprehensive security analysis.</p>
        </div>
        """, unsafe_allow_html=True)
        return
    
    # Security configuration overview
    st.markdown("**üõ°Ô∏è Current Security Configuration:**")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("**üîê Encryption Status:**")
        encryption_score = 0
        if config.get('encryption_at_rest', False):
            st.success("‚úÖ Encryption at Rest")
            encryption_score += 1
        else:
            st.error("‚ùå Encryption at Rest")
        
        if config.get('encryption_in_transit', False):
            st.success("‚úÖ Encryption in Transit")
            encryption_score += 1
        else:
            st.error("‚ùå Encryption in Transit")
    
    with col2:
        st.markdown("**üîë Access Controls:**")
        access_score = 0
        if config.get('iam_auth', False):
            st.success("‚úÖ IAM Authentication")
            access_score += 1
        else:
            st.warning("‚ö†Ô∏è IAM Authentication")
        
        if config.get('multi_az', False):
            st.success("‚úÖ Multi-AZ Deployment")
            access_score += 1
        else:
            st.warning("‚ö†Ô∏è Single-AZ Deployment")
    
    with col3:
        st.markdown("**üìã Compliance:**")
        compliance_reqs = config.get('compliance_requirements', [])
        if compliance_reqs:
            for req in compliance_reqs:
                st.info(f"üìã {req} Required")
        else:
            st.warning("‚ö†Ô∏è No Compliance Requirements")
    
    if st.button("üîç Run Comprehensive Security Analysis", type="primary", key="run_security_analysis"):
        with st.spinner("üîí Analyzing security posture and compliance..."):
            
            # Create migration context for security analysis
            migration_context = {
                'source_engine': config['source_engine'],
                'target_engine': config['target_engine'],
                'schema_ddl': schema_ddl,
                'encryption_at_rest': config.get('encryption_at_rest', False),
                'encryption_in_transit': config.get('encryption_in_transit', False),
                'iam_enabled': config.get('iam_auth', False),
                'vpc_enabled': True,  # Assume VPC for AWS
                'audit_logging': True,  # AWS default
                'compliance_requirements': config.get('compliance_requirements', []),
                'business_critical': config.get('business_critical', False)
            }
            
            # Run security analysis
            security_analyzer = SecurityAnalyzer()
            security_assessment = security_analyzer.analyze_security(migration_context)
            
            # Store results
            st.session_state.security_assessment = security_assessment
            
            # Display results
            st.markdown("**üîí Security Analysis Results:**")
            
            # Overall security score
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                score_color = "success" if security_assessment.overall_score >= 80 else "warning" if security_assessment.overall_score >= 60 else "error"
                if score_color == "success":
                    st.success(f"üîí Security Score: {security_assessment.overall_score:.0f}/100")
                elif score_color == "warning":
                    st.warning(f"üîí Security Score: {security_assessment.overall_score:.0f}/100")
                else:
                    st.error(f"üîí Security Score: {security_assessment.overall_score:.0f}/100")
            
            with col2:
                vuln_count = len(security_assessment.vulnerabilities)
                high_vuln = sum(1 for v in security_assessment.vulnerabilities if v['severity'] == 'high')
                if high_vuln > 0:
                    st.error(f"üö® {vuln_count} Vulnerabilities ({high_vuln} High)")
                elif vuln_count > 0:
                    st.warning(f"‚ö†Ô∏è {vuln_count} Vulnerabilities")
                else:
                    st.success("‚úÖ No Critical Vulnerabilities")
            
            with col3:
                compliance_passed = sum(1 for passed in security_assessment.compliance_status.values() if passed)
                compliance_total = len(security_assessment.compliance_status)
                if compliance_passed == compliance_total:
                    st.success(f"‚úÖ {compliance_passed}/{compliance_total} Compliance")
                else:
                    st.error(f"‚ùå {compliance_passed}/{compliance_total} Compliance")
            
            with col4:
                pii_tables = sum(1 for classification in security_assessment.data_classification.values() if classification == 'pii')
                if pii_tables > 0:
                    st.warning(f"üîê {pii_tables} PII Tables Detected")
                else:
                    st.success("‚úÖ No PII Detected")
            
            # Vulnerability details
            if security_assessment.vulnerabilities:
                st.markdown("**üö® Security Vulnerabilities:**")
                
                for i, vuln in enumerate(security_assessment.vulnerabilities):
                    severity_color = {'high': 'error', 'medium': 'warning', 'low': 'info'}
                    
                    with st.expander(f"{vuln['severity'].upper()}: {vuln['description']}", expanded=vuln['severity'] == 'high'):
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            st.markdown(f"**Type:** {vuln['type']}")
                            st.markdown(f"**Severity:** {vuln['severity']}")
                        
                        with col2:
                            st.markdown(f"**Recommendation:** {vuln['recommendation']}")
            
            # Compliance status
            st.markdown("**üìã Compliance Status:**")
            
            compliance_data = []
            for framework, status in security_assessment.compliance_status.items():
                compliance_data.append({
                    'Framework': framework,
                    'Status': 'Compliant' if status else 'Non-Compliant',
                    'Color': '#22c55e' if status else '#ef4444'
                })
            
            if compliance_data:
                df = pd.DataFrame(compliance_data)
                fig = px.bar(df, x='Framework', y=['Status'], 
                           color='Status', color_discrete_map={'Compliant': '#22c55e', 'Non-Compliant': '#ef4444'},
                           title='Compliance Framework Status')
                fig.update_layout(height=300)
                st.plotly_chart(fig, use_container_width=True, key="compiance_framework_status")
            
            # Data classification results
            st.markdown("**üìä Data Classification Results:**")
            
            classification_counts = {}
            for table, classification in security_assessment.data_classification.items():
                classification_counts[classification] = classification_counts.get(classification, 0) + 1
            
            if classification_counts:
                class_df = pd.DataFrame({
                    'Classification': list(classification_counts.keys()),
                    'Count': list(classification_counts.values())
                })
                
                fig = px.pie(class_df, values='Count', names='Classification',
                           title='Data Classification Distribution',
                           color_discrete_map={
                               'public': '#22c55e',
                               'pii': '#f59e0b', 
                               'sensitive': '#ef4444'
                           })
                fig.update_layout(height=400)
                st.plotly_chart(fig, use_container_width=True, key="data_classification_distribution")
            
            # Security recommendations
            st.markdown("**üí° Security Recommendations:**")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown(f"""
                <div class="security-card">
                    <h4>üéØ Immediate Actions</h4>
                    {''.join([f'<p>‚Ä¢ {rec}</p>' for rec in security_assessment.recommendations[:4]])}
                </div>
                """, unsafe_allow_html=True)
            
            with col2:
                # Generate AWS security best practices
                aws_recommendations = [
                    "Enable AWS CloudTrail for audit logging",
                    "Configure VPC Flow Logs for network monitoring",
                    "Implement AWS Secrets Manager for credentials",
                    "Enable AWS GuardDuty for threat detection",
                    "Configure AWS Config for compliance monitoring"
                ]
                
                st.markdown(f"""
                <div class="enterprise-card">
                    <h4>‚òÅÔ∏è AWS Security Best Practices</h4>
                    {''.join([f'<p>‚Ä¢ {rec}</p>' for rec in aws_recommendations[:4]])}
                </div>
                """, unsafe_allow_html=True)

def render_aws_mapping_tab(config: Dict):
    """Enhanced AWS service mapping"""
    st.subheader("‚òÅÔ∏è Enhanced AWS Service Mapping & Recommendations")
    st.markdown('<span class="feature-badge badge-enhanced">‚òÅÔ∏è AWS Integration</span>', unsafe_allow_html=True)
    
    # Enhanced AWS service recommendations
    source_info = get_database_info(config['source_engine'])
    
    st.markdown(f"""
    <div class="enterprise-card">
        <h4>‚òÅÔ∏è AWS Migration Architecture</h4>
        <p><strong>Source:</strong> {source_info['icon']} {source_info['display_name']} ‚Üí <strong>Target:</strong> ‚òÅÔ∏è AWS {config['target_engine'].replace('_', ' ').title()}</p>
        <p><strong>Recommended AWS Services:</strong> RDS/Aurora, DMS, CloudWatch, VPC, IAM</p>
    </div>
    """, unsafe_allow_html=True)
    
    # AWS service mapping with enhanced features
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("""
        <div class="cost-card">
            <h4>üóÑÔ∏è Database Services</h4>
            <p><strong>Primary:</strong> Aurora PostgreSQL</p>
            <p><strong>Alternative:</strong> RDS PostgreSQL</p>
            <p><strong>Benefits:</strong></p>
            <p>‚Ä¢ 3x faster than standard PostgreSQL</p>
            <p>‚Ä¢ Automated backup & recovery</p>
            <p>‚Ä¢ Global database support</p>
            <p>‚Ä¢ Serverless option available</p>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown("""
        <div class="enterprise-card">
            <h4>üöö Migration Services</h4>
            <p><strong>Primary:</strong> AWS DMS</p>
            <p><strong>Features:</strong></p>
            <p>‚Ä¢ Minimal downtime migration</p>
            <p>‚Ä¢ Continuous data replication</p>
            <p>‚Ä¢ Schema conversion support</p>
            <p>‚Ä¢ Real-time monitoring</p>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        st.markdown("""
        <div class="security-card">
            <h4>üîí Security & Monitoring</h4>
            <p><strong>Security:</strong> VPC, IAM, KMS</p>
            <p><strong>Monitoring:</strong> CloudWatch, Performance Insights</p>
            <p><strong>Compliance:</strong> AWS Config, CloudTrail</p>
            <p><strong>Backup:</strong> Automated, Point-in-time recovery</p>
        </div>
        """, unsafe_allow_html=True)

def render_migration_scripts_tab(config: Dict, schema_ddl: str):
    """Enhanced migration scripts generation"""
    st.subheader("üìú Enhanced Migration Scripts Generation")
    st.markdown('<span class="feature-badge badge-enhanced">üìú Script Generation</span>', unsafe_allow_html=True)
    
    if not schema_ddl:
        st.markdown("""
        <div class="warning-banner">
            <h4>‚ö†Ô∏è Schema Required for Script Generation</h4>
            <p>Please provide schema DDL in the Schema Input tab to generate comprehensive migration scripts.</p>
        </div>
        """, unsafe_allow_html=True)
        return
    
    source_info = get_database_info(config['source_engine'])
    
    st.markdown(f"""
    <div class="enterprise-card">
        <h4>üìú Enterprise Migration Scripts</h4>
        <p>Generate comprehensive migration scripts for {source_info['display_name']} ‚Üí AWS {config['target_engine'].replace('_', ' ').title()}</p>
        <p>Includes pre-migration, conversion, post-migration, and validation scripts</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Enhanced script generation options
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("**üìã Pre-Migration Scripts:**")
        generate_pre = st.checkbox("Pre-Migration Validation", True, key="scripts_generate_pre")
        include_backup = st.checkbox("Backup Scripts", True, key="scripts_include_backup")
        include_validation = st.checkbox("Data Validation", True, key="scripts_include_validation")
    
    with col2:
        st.markdown("**üîÑ Conversion Scripts:**")
        generate_conversion = st.checkbox("Schema Conversion", True, key="scripts_generate_conversion")
        include_indexes = st.checkbox("Index Creation", True, key="scripts_include_indexes")
        include_constraints = st.checkbox("Constraint Migration", True, key="scripts_include_constraints")
    
    with col3:
        st.markdown("**‚úÖ Post-Migration Scripts:**")
        generate_post = st.checkbox("Post-Migration Validation", True, key="scripts_generate_post")
        include_optimization = st.checkbox("Performance Optimization", True, key="scripts_include_optimization")
        include_monitoring = st.checkbox("Monitoring Setup", True, key="scripts_include_monitoring")
    
    if st.button("üöÄ Generate Enterprise Migration Scripts", type="primary", key="generate_migration_scripts"):
        with st.spinner("üìù Generating comprehensive migration scripts..."):
            time.sleep(2)  # Simulate script generation
            
            # Display generated scripts
            st.markdown("**üìú Generated Migration Scripts:**")
            
            # Pre-migration script
            if generate_pre:
                with st.expander("üìã Pre-Migration Validation Script", expanded=False):
                    pre_script = f"""-- Enhanced Pre-Migration Validation Script
-- Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
-- Migration: {source_info['display_name']} ‚Üí AWS {config['target_engine'].replace('_', ' ').title()}

-- 1. Environment Validation
SELECT 'Pre-migration validation started' as status, NOW() as timestamp;

-- 2. Source Database Health Check
-- (Database-specific health checks would be generated based on source engine)

-- 3. Table Count and Size Analysis
-- (Generated based on actual schema analysis)

-- 4. Data Integrity Checks
-- Add specific validation queries based on schema analysis

SELECT 'Pre-migration validation completed' as status, NOW() as timestamp;"""
                    
                    st.code(pre_script, language='sql')
                    st.download_button(
                        "üì• Download Pre-Migration Script",
                        pre_script,
                        f"pre_migration_{config['source_engine']}_to_{config['target_engine']}.sql",
                        "text/sql",
                        key="download_pre_script"
                    )
            
            # Schema conversion script
            if generate_conversion:
                with st.expander("üîÑ Schema Conversion Script", expanded=False):
                    conversion_script = f"""-- Enhanced Schema Conversion Script
-- Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
-- Migration: {source_info['display_name']} ‚Üí AWS {config['target_engine'].replace('_', ' ').title()}

-- 1. Create target database
-- (AWS-specific database creation commands)

-- 2. Enhanced table creation with AWS optimizations
-- (Tables would be converted based on actual schema analysis)

-- 3. Index creation with AWS optimizations
-- (Generated based on source schema analysis)

-- 4. Enhanced constraints and foreign keys
-- (Generated based on source schema analysis)

SELECT 'Schema conversion completed' as status, NOW() as timestamp;"""
                    
                    st.code(conversion_script, language='sql')
                    st.download_button(
                        "üì• Download Conversion Script",
                        conversion_script,
                        f"schema_conversion_{config['source_engine']}_to_{config['target_engine']}.sql",
                        "text/sql",
                        key="download_conversion_script"
                    )
            
            # Post-migration script
            if generate_post:
                with st.expander("‚úÖ Post-Migration Validation Script", expanded=False):
                    post_script = f"""-- Enhanced Post-Migration Validation Script
-- Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
-- Migration: {source_info['display_name']} ‚Üí AWS {config['target_engine'].replace('_', ' ').title()}

-- 1. Data validation and verification
SELECT 'Post-migration validation started' as status, NOW() as timestamp;

-- 2. Row count validation
-- (Generated based on source tables)

-- 3. Data integrity checks
-- (Generated based on schema analysis)

-- 4. Performance baseline establishment
-- (AWS-specific optimization commands)

-- 5. AWS-specific optimizations
-- Configure CloudWatch metrics
-- Set up automated backups

SELECT 'Post-migration validation completed' as status, NOW() as timestamp;
SELECT 'Migration ready for production use' as final_status;"""
                    
                    st.code(post_script, language='sql')
                    st.download_button(
                        "üì• Download Post-Migration Script",
                        post_script,
                        f"post_migration_{config['source_engine']}_to_{config['target_engine']}.sql",
                        "text/sql",
                        key="download_post_script"
                    )
            
            # Enhanced migration checklist
            st.markdown("**üìã Enterprise Migration Checklist:**")
            
            checklist = f"""# Enterprise Migration Execution Checklist

## üìã Pre-Migration Phase
- [ ] **Environment Setup**
  - [ ] AWS account configured with proper permissions
  - [ ] VPC and security groups configured
  - [ ] Target database instance provisioned
  - [ ] DMS replication instance created

- [ ] **Security Configuration**
  - [ ] Encryption at rest enabled: {config.get('encryption_at_rest', False)}
  - [ ] Encryption in transit configured: {config.get('encryption_in_transit', False)}
  - [ ] IAM authentication setup: {config.get('iam_auth', False)}
  - [ ] Compliance requirements validated: {', '.join(config.get('compliance_requirements', []))}

## üöÄ Migration Execution Phase
- [ ] **Migration Process**
  - [ ] DMS endpoints configured and tested
  - [ ] Full load migration initiated
  - [ ] CDC (Change Data Capture) enabled
  - [ ] Data validation during migration

## ‚úÖ Post-Migration Phase
- [ ] **Validation & Testing**
  - [ ] Data consistency verification completed
  - [ ] Application functionality testing passed
  - [ ] Performance testing completed

- [ ] **Production Readiness**
  - [ ] Monitoring and alerting configured
  - [ ] Backup schedules established
  - [ ] Documentation updated

## üéØ Success Criteria
- [ ] Zero data loss during migration
- [ ] Application downtime < {config.get('downtime_tolerance', 'Target SLA')}
- [ ] Performance metrics meet or exceed baseline
- [ ] All compliance requirements satisfied
"""
            
            st.markdown(f"""
            <div class="enterprise-card">
                <h4>‚úÖ Enterprise Migration Checklist</h4>
                <div style="max-height: 400px; overflow-y: auto;">
                    <pre>{checklist}</pre>
                </div>
            </div>
            """, unsafe_allow_html=True)
            
            # Download checklist
            st.download_button(
                "üì• Download Migration Checklist",
                checklist,
                f"migration_checklist_{config['source_engine']}_to_{config['target_engine']}.md",
                "text/markdown",
                key="download_checklist"
            )

def render_enhanced_ai_analysis_tab(config: Dict, migration_context: Dict):
    """Render enhanced AI analysis with comprehensive insights"""
    st.subheader("ü§ñ Enhanced AI Migration Analysis")
    
    st.markdown('<span class="feature-badge badge-ai">ü§ñ AI-Powered Intelligence</span>', unsafe_allow_html=True)
    
    ai_analyzer = EnterpriseAIAnalyzer()
    
    if not ai_analyzer.connected:
        st.markdown("""
        <div class="warning-banner">
            <h4>‚ö†Ô∏è AI Analysis Service</h4>
            <p>AI analysis running in demo mode. Configure ANTHROPIC_API_KEY for full AI capabilities.</p>
        </div>
        """, unsafe_allow_html=True)
    
    # AI Analysis configuration
    st.markdown("**üéØ AI Analysis Configuration:**")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        analysis_types = st.multiselect(
            "Select Analysis Types",
            options=[
                "Migration Strategy",
                "Risk Assessment", 
                "Cost Optimization",
                "Timeline Estimation",
                "Security Analysis"
            ],
            default=["Migration Strategy", "Risk Assessment", "Timeline Estimation"],
            key="ai_analysis_types"
        )
    
    with col2:
        analysis_depth = st.selectbox("Analysis Depth", ["Standard", "Comprehensive", "Expert"], key="ai_analysis_depth")
        include_industry_context = st.checkbox("Include Industry Best Practices", True, key="ai_include_industry")
    
    with col3:
        team_experience = st.selectbox("Team Experience Level", ["Beginner", "Intermediate", "Expert"], key="ai_team_experience")
        budget_constraints = st.selectbox("Budget Flexibility", ["Tight", "Moderate", "Flexible"], key="ai_budget_constraints")
    
    if st.button("üß† Run Enhanced AI Analysis", type="primary", key="run_ai_analysis"):
        with st.spinner("ü§ñ Running comprehensive AI analysis..."):
            
            try:
                # Enhanced migration context
                enhanced_context = {
                    **migration_context,
                    'analysis_depth': analysis_depth,
                    'team_experience': team_experience,
                    'budget_constraints': budget_constraints,
                    'include_industry_context': include_industry_context,
                    'business_critical': config.get('business_critical', False),
                    'compliance_requirements': config.get('compliance_requirements', []),
                    'downtime_tolerance': config.get('downtime_tolerance', 'Unknown')
                }
                
                # Run comprehensive AI analysis
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                analysis_results = loop.run_until_complete(
                    ai_analyzer.comprehensive_analysis(enhanced_context)
                )
                loop.close()
                
                # Store results
                st.session_state.analysis_results.update(analysis_results)
                
                # Display executive summary first
                st.markdown("**üìã AI Analysis Executive Summary:**")
                
                # Calculate overall metrics
                overall_confidence = sum(r.confidence_score for r in analysis_results.values()) / len(analysis_results)
                total_risks = sum(len(r.risks) for r in analysis_results.values())
                total_recommendations = sum(len(r.recommendations) for r in analysis_results.values())
                
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    confidence_color = "success" if overall_confidence > 0.8 else "warning" if overall_confidence > 0.6 else "error"
                    if confidence_color == "success":
                        st.success(f"üéØ Overall Confidence: {overall_confidence:.1%}")
                    elif confidence_color == "warning":
                        st.warning(f"üéØ Overall Confidence: {overall_confidence:.1%}")
                    else:
                        st.error(f"üéØ Overall Confidence: {overall_confidence:.1%}")
                
                with col2:
                    if total_risks <= 3:
                        st.success(f"‚ö†Ô∏è {total_risks} Risks Identified")
                    elif total_risks <= 6:
                        st.warning(f"‚ö†Ô∏è {total_risks} Risks Identified")
                    else:
                        st.error(f"‚ö†Ô∏è {total_risks} Risks Identified")
                
                with col3:
                    st.info(f"üí° {total_recommendations} Recommendations")
                
                with col4:
                    migration_readiness = "Ready" if overall_confidence > 0.7 else "Needs Planning" if overall_confidence > 0.5 else "High Risk"
                    readiness_color = "success" if migration_readiness == "Ready" else "warning" if migration_readiness == "Needs Planning" else "error"
                    
                    if readiness_color == "success":
                        st.success(f"üöÄ Status: {migration_readiness}")
                    elif readiness_color == "warning":
                        st.warning(f"üöÄ Status: {migration_readiness}")
                    else:
                        st.error(f"üöÄ Status: {migration_readiness}")
                
                # Detailed analysis results
                for analysis_type, result in analysis_results.items():
                    type_name = analysis_type.value.replace('_', ' ').title()
                    if type_name.replace(' ', '_').lower() in [a.replace(' ', '_').lower() for a in analysis_types]:
                        
                        with st.expander(f"üìä {type_name} Analysis", expanded=True):
                            
                            # Analysis metrics
                            col1, col2, col3, col4 = st.columns(4)
                            
                            with col1:
                                confidence_color = "success" if result.confidence_score > 0.8 else "warning"
                                st.metric("üéØ Confidence", f"{result.confidence_score:.1%}")
                            
                            with col2:
                                st.metric("‚ö†Ô∏è Risks", len(result.risks))
                            
                            with col3:
                                st.metric("üí° Recommendations", len(result.recommendations))
                            
                            with col4:
                                st.metric("üéØ Action Items", len(result.action_items))
                            
                            # Analysis content
                            col1, col2 = st.columns(2)
                            
                            with col1:
                                if result.recommendations:
                                    st.markdown("**üí° Key Recommendations:**")
                                    for i, rec in enumerate(result.recommendations[:5]):
                                        st.write(f"{i+1}. {rec}")
                                
                                if result.opportunities:
                                    st.markdown("**üöÄ Opportunities:**")
                                    for opp in result.opportunities[:3]:
                                        st.write(f"‚Ä¢ {opp}")
                            
                            with col2:
                                if result.risks:
                                    st.markdown("**‚ö†Ô∏è Key Risks:**")
                                    for risk in result.risks[:5]:
                                        severity_icons = {'high': 'üî¥', 'medium': 'üü°', 'low': 'üü¢'}
                                        st.write(f"{severity_icons.get(risk['severity'], 'üîµ')} {risk['description']}")
                                
                                if result.action_items:
                                    st.markdown("**üéØ Priority Actions:**")
                                    for item in result.action_items[:3]:
                                        priority_icons = {'high': 'üî¥', 'medium': 'üü°', 'low': 'üü¢'}
                                        st.write(f"{priority_icons.get(item['priority'], 'üîµ')} {item['action']}")
                            
                            # Timeline and cost impact
                            col1, col2 = st.columns(2)
                            
                            with col1:
                                st.markdown(f"""
                                <div class="metric-card">
                                    <h5>‚è±Ô∏è Timeline Estimate</h5>
                                    <p>{result.timeline_estimate}</p>
                                </div>
                                """, unsafe_allow_html=True)
                            
                            with col2:
                                st.markdown(f"""
                                <div class="metric-card">
                                    <h5>üí∞ Cost Impact</h5>
                                    <p>{result.cost_impact}</p>
                                </div>
                                """, unsafe_allow_html=True)
                            
                            # Detailed analysis (collapsible)
                            with st.expander("üìÑ Detailed AI Analysis", expanded=False):
                                st.markdown(result.detailed_analysis)
                
                # Final recommendations summary
                st.markdown("**üéØ Final AI Recommendations:**")
                
                # Collect high-priority actions from all analyses
                high_priority_actions = []
                for result in analysis_results.values():
                    high_priority_actions.extend([
                        item['action'] for item in result.action_items 
                        if item['priority'] == 'high'
                    ])
                
                if high_priority_actions:
                    st.markdown(f"""
                    <div class="ai-enhanced-card">
                        <h4>üî¥ Immediate Actions Required</h4>
                        {''.join([f'<p>‚Ä¢ {action}</p>' for action in high_priority_actions[:5]])}
                    </div>
                    """, unsafe_allow_html=True)
                
                # Overall recommendation
                if overall_confidence > 0.8:
                    st.markdown("""
                    <div class="success-banner">
                        <h4>‚úÖ Migration Recommendation: PROCEED</h4>
                        <p>AI analysis indicates this migration is well-positioned for success with proper execution of the recommended strategy.</p>
                    </div>
                    """, unsafe_allow_html=True)
                elif overall_confidence > 0.6:
                    st.markdown("""
                    <div class="warning-banner">
                        <h4>‚ö†Ô∏è Migration Recommendation: PROCEED WITH CAUTION</h4>
                        <p>Migration is feasible but requires careful planning and risk mitigation. Address high-priority recommendations before proceeding.</p>
                    </div>
                    """, unsafe_allow_html=True)
                else:
                    st.markdown("""
                    <div class="error-banner">
                        <h4>üî¥ Migration Recommendation: ADDITIONAL PLANNING REQUIRED</h4>
                        <p>Significant risks and complexities identified. Extensive planning and risk assessment required before migration.</p>
                    </div>
                    """, unsafe_allow_html=True)
                
            except Exception as e:
                st.error(f"AI analysis failed: {e}")
                st.info("Please check your configuration and try again.")

def render_enhanced_autofix_tab(config: Dict, schema_ddl: str, queries_text: str):
    """Render the enhanced auto-fix analysis tab"""
    st.subheader("üîß Enhanced Auto-Fix Engine")
    
    st.markdown('<span class="feature-badge badge-new">üîß Auto-Fix Capabilities</span>', unsafe_allow_html=True)
    
    if not schema_ddl and not queries_text:
        st.markdown("""
        <div class="warning-banner">
            <h4>‚ö†Ô∏è Input Required for Auto-Fix</h4>
            <p>Please provide schema DDL or queries in the Schema Input tab to run auto-fix analysis.</p>
        </div>
        """, unsafe_allow_html=True)
        return
    
    # Auto-fix configuration
    st.markdown("**üéØ Auto-Fix Configuration:**")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("**üîß Fix Categories:**")
        fix_categories = []
        if st.checkbox("Syntax & Compatibility", True, key="autofix_syntax_compat"):
            fix_categories.extend([FixCategory.SYNTAX, FixCategory.COMPATIBILITY])
        if st.checkbox("Performance Optimization", True, key="autofix_performance"):
            fix_categories.append(FixCategory.PERFORMANCE)
        if st.checkbox("Security Enhancements", True, key="autofix_security"):
            fix_categories.append(FixCategory.SECURITY)
        if st.checkbox("Compliance & Governance", False, key="autofix_compliance"):
            fix_categories.append(FixCategory.COMPLIANCE)
    
    with col2:
        st.markdown("**‚öôÔ∏è Auto-Apply Settings:**")
        auto_apply_safe = st.checkbox("Auto-apply safe fixes", False, key="autofix_auto_apply")
        min_confidence = st.slider("Minimum confidence for auto-apply", 0.5, 1.0, 0.9, 0.05, key="autofix_min_confidence")
        show_cosmetic_fixes = st.checkbox("Show cosmetic fixes", False, key="autofix_show_cosmetic")
    
    with col3:
        st.markdown("**üé® Display Options:**")
        show_diff_view = st.checkbox("Show diff view", True, key="autofix_show_diff")
        group_by_severity = st.checkbox("Group by severity", True, key="autofix_group_severity")
        show_ai_explanations = st.checkbox("Show AI explanations", True, key="autofix_show_ai")
    
    if st.button("üöÄ Run Auto-Fix Analysis", type="primary", key="run_autofix_analysis"):
        with st.spinner("üîç Analyzing code and generating fixes..."):
            
            # Initialize auto-fix engine
            autofix_engine = EnterpriseAutoFixEngine()
            
            # Run comprehensive auto-fix analysis
            fix_result = autofix_engine.analyze_and_fix(
                source_engine=config['source_engine'],
                target_engine=config['target_engine'],
                schema_ddl=schema_ddl,
                queries=queries_text,
                fix_categories=fix_categories,
                auto_apply_safe=auto_apply_safe
            )
            
            # Store results in session state
            st.session_state.autofix_results = fix_result
            
            # Display results summary
            st.markdown("**üìä Auto-Fix Analysis Results:**")
            
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                if fix_result.critical_issues > 0:
                    st.error(f"üö® {fix_result.critical_issues} Critical Issues")
                else:
                    st.success("‚úÖ No Critical Issues")
            
            with col2:
                st.info(f"üîß {fix_result.fixes_available} Fixes Available")
            
            with col3:
                if fix_result.fixes_applied > 0:
                    st.success(f"‚úÖ {fix_result.fixes_applied} Auto-Applied")
                else:
                    st.warning("‚è≥ No Fixes Applied")
            
            with col4:
                score_improvement = fix_result.compatibility_score_after - fix_result.compatibility_score_before
                st.metric(
                    "üéØ Compatibility Score",
                    f"{fix_result.compatibility_score_after:.0f}%",
                    delta=f"+{score_improvement:.0f}%" if score_improvement > 0 else None
                )
            
            # Display fixes by category
            if fix_result.fixes:
                
                if group_by_severity:
                    # Group fixes by severity
                    severity_groups = {}
                    for fix in fix_result.fixes:
                        if fix.severity not in severity_groups:
                            severity_groups[fix.severity] = []
                        severity_groups[fix.severity].append(fix)
                    
                    # Display in severity order
                    severity_order = [FixSeverity.CRITICAL, FixSeverity.HIGH, FixSeverity.MEDIUM, FixSeverity.LOW]
                    if show_cosmetic_fixes:
                        severity_order.append(FixSeverity.COSMETIC)
                    
                    for severity in severity_order:
                        if severity in severity_groups:
                            fixes_in_group = severity_groups[severity]
                            
                            # Severity header with styling
                            severity_colors = {
                                FixSeverity.CRITICAL: "üî¥",
                                FixSeverity.HIGH: "üü†", 
                                FixSeverity.MEDIUM: "üü°",
                                FixSeverity.LOW: "üü¢",
                                FixSeverity.COSMETIC: "üîµ"
                            }
                            
                            st.markdown(f"**{severity_colors[severity]} {severity.value.title()} Priority Fixes ({len(fixes_in_group)}):**")
                            
                            for i, fix in enumerate(fixes_in_group):
                                render_fix_item(fix, i, show_diff_view, show_ai_explanations, autofix_engine, schema_ddl, queries_text)
                
                else:
                    # Display all fixes together
                    st.markdown(f"**üîß All Available Fixes ({len(fix_result.fixes)}):**")
                    for i, fix in enumerate(fix_result.fixes):
                        render_fix_item(fix, i, show_diff_view, show_ai_explanations, autofix_engine, schema_ddl, queries_text)
            
            # Summary and next steps
            st.markdown("**üìã Summary & Next Steps:**")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown(f"""
                <div class="analysis-card">
                    <h4>üìä Analysis Summary</h4>
                    <pre>{fix_result.summary_report}</pre>
                </div>
                """, unsafe_allow_html=True)
            
            with col2:
                if fix_result.fixes_available > 0:
                    unapplied_fixes = [f for f in fix_result.fixes if f.status == FixStatus.PENDING]
                    if unapplied_fixes:
                        st.markdown(f"""
                        <div class="enterprise-card">
                            <h4>üéØ Recommended Actions</h4>
                            <p>‚Ä¢ Review {len(unapplied_fixes)} pending fixes</p>
                            <p>‚Ä¢ Apply critical and high-priority fixes first</p>
                            <p>‚Ä¢ Test fixes in staging environment</p>
                            <p>‚Ä¢ Update application code if needed</p>
                        </div>
                        """, unsafe_allow_html=True)
                
                # Performance and security insights
                if fix_result.performance_gains != "No performance issues detected":
                    st.info(f"‚ö° {fix_result.performance_gains}")
                
                if fix_result.security_improvements:
                    st.warning(f"üîí {len(fix_result.security_improvements)} security improvements available")
            
            # Download fixed code
            if fix_result.fixes_applied > 0:
                applied_fixes = [f for f in fix_result.fixes if f.status == FixStatus.APPLIED]
                fixed_result = autofix_engine.apply_fixes(applied_fixes, schema_ddl, queries_text)
                
                st.markdown("**üì• Download Fixed Code:**")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    if fixed_result['fixed_schema']:
                        st.download_button(
                            "üì• Download Fixed Schema",
                            fixed_result['fixed_schema'],
                            f"fixed_schema_{config['source_engine']}_to_{config['target_engine']}.sql",
                            "text/sql",
                            key="download_fixed_schema"
                        )
                
                with col2:
                    if fixed_result['fixed_queries']:
                        st.download_button(
                            "üì• Download Fixed Queries", 
                            fixed_result['fixed_queries'],
                            f"fixed_queries_{config['source_engine']}_to_{config['target_engine']}.sql",
                            "text/sql",
                            key="download_fixed_queries"
                        )

def render_fix_item(fix: AutoFix, index: int, show_diff_view: bool, show_ai_explanations: bool, 
                   autofix_engine, schema_ddl: str, queries_text: str):
    """Render individual fix item with interactive controls"""
    
    # Status indicators
    status_icons = {
        FixStatus.PENDING: "‚è≥",
        FixStatus.APPLIED: "‚úÖ", 
        FixStatus.FAILED: "‚ùå",
        FixStatus.SKIPPED: "‚è≠Ô∏è",
        FixStatus.REVIEWED: "üëÅÔ∏è"
    }
    
    category_icons = {
        FixCategory.SYNTAX: "üìù",
        FixCategory.COMPATIBILITY: "üîÑ",
        FixCategory.PERFORMANCE: "‚ö°",
        FixCategory.SECURITY: "üîí",
        FixCategory.OPTIMIZATION: "üéØ",
        FixCategory.COMPLIANCE: "üìã"
    }
    
    # Expandable fix item
    with st.expander(
        f"{status_icons[fix.status]} {category_icons[fix.category]} {fix.title} (Confidence: {fix.confidence_score:.1%})",
        expanded=fix.severity in [FixSeverity.CRITICAL, FixSeverity.HIGH]
    ):
        
        # Fix details
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.markdown(f"**Description:** {fix.description}")
            if show_ai_explanations:
                st.markdown(f"**Explanation:** {fix.explanation}")
            st.markdown(f"**Impact:** {fix.estimated_impact}")
            
            if fix.warnings:
                for warning in fix.warnings:
                    st.warning(f"‚ö†Ô∏è {warning}")
            
            if fix.prerequisites:
                st.info(f"**Prerequisites:** {', '.join(fix.prerequisites)}")
        
        with col2:
            st.metric("Confidence", f"{fix.confidence_score:.1%}")
            st.metric("Category", fix.category.value.title())
            st.metric("Severity", fix.severity.value.title())
        
        # Code comparison
        if show_diff_view and fix.original_code and fix.fixed_code:
            st.markdown("**Code Changes:**")
            
            # Show diff using difflib
            diff_lines = list(difflib.unified_diff(
                fix.original_code.splitlines(keepends=True),
                fix.fixed_code.splitlines(keepends=True),
                fromfile="Original",
                tofile="Fixed",
                n=3
            ))
            
            if diff_lines:
                diff_text = ''.join(diff_lines)
                st.code(diff_text, language='diff')
            else:
                col1, col2 = st.columns(2)
                with col1:
                    st.markdown("**Original:**")
                    st.code(fix.original_code, language='sql')
                with col2:
                    st.markdown("**Fixed:**")
                    st.code(fix.fixed_code, language='sql')
        
        # Action buttons
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            if st.button(f"‚úÖ Apply Fix", key=f"apply_{fix.id}_{index}"):
                fix.status = FixStatus.APPLIED
                st.success("Fix applied!")
                st.rerun()
        
        with col2:
            if st.button(f"‚è≠Ô∏è Skip Fix", key=f"skip_{fix.id}_{index}"):
                fix.status = FixStatus.SKIPPED
                st.info("Fix skipped")
                st.rerun()
        
        with col3:
            if st.button(f"üëÅÔ∏è Mark Reviewed", key=f"review_{fix.id}_{index}"):
                fix.status = FixStatus.REVIEWED
                st.info("Fix marked as reviewed")
                st.rerun()
        
        with col4:
            if st.button(f"üìã Copy Fixed Code", key=f"copy_{fix.id}_{index}"):
                st.code(fix.fixed_code, language='sql')

def main():
    """Enhanced main application function"""
    # Initialize session state
    initialize_session_state()
    
    # Render enterprise header
    render_enterprise_header()
    
    # Get enhanced configuration from sidebar
    config = render_enhanced_sidebar()
    
    # Create project if requested
    if st.session_state.get('show_project_creator', False):
        with st.sidebar.form("create_project"):
            st.markdown("**Create New Project:**")
            project_name = st.text_input("Project Name", key="project_name_input")
            project_description = st.text_area("Description", key="project_description_input")
            
            if st.form_submit_button("Create Project"):
                db_manager = EnterpriseDBManager()
                project_id = db_manager.create_project(
                    project_name, 
                    config['source_engine'], 
                    config['target_engine'], 
                    st.session_state.user_id
                )
                st.session_state.current_project = project_id
                st.session_state.show_project_creator = False
                st.success(f"Project '{project_name}' created successfully!")
                st.rerun()
    
    # Enhanced main content tabs - ADD the auto-fix tab
    tab1, tab2, tab3, tab4, tab5, tab6, tab7, tab8, tab9, tab10 = st.tabs([
        "üìä Dashboard",
        "üìö Examples & Tutorials", 
        "üìã Schema Input & Analysis",
        "üîç Compatibility Analysis",
        "üîß Auto-Fix Engine",          # Auto-fix capabilities
        "üí∞ Cost Analysis",
        "üîí Security Analysis",
        "‚òÅÔ∏è AWS Service Mapping",
        "üìú Migration Scripts",
        "ü§ñ AI Analysis"
    ])
    
    with tab1:
        render_enterprise_dashboard_tab()
    
    with tab2:
        render_examples_tab()
    
    with tab3:
        schema_ddl, queries_text = render_schema_input_tab(config)
        
        # Enhanced input summary
        if schema_ddl or queries_text:
            st.markdown("**üìä Input Summary:**")
            col1, col2, col3 = st.columns(3)
            
            source_info = DATABASE_CONFIG[config['source_engine']]
            
            with col1:
                if schema_ddl:
                    schema_size = len(schema_ddl)
                    st.success(f"‚úÖ {source_info['schema_label']} provided ({schema_size:,} characters)")
                    
                    # Analyze schema complexity
                    table_count = len(re.findall(r'CREATE TABLE', schema_ddl, re.IGNORECASE))
                    if table_count > 0:
                        st.info(f"üìä {table_count} tables detected")
            
            with col2:
                if queries_text:
                    query_count = len([q for q in queries_text.split(';') if q.strip()])
                    st.success(f"‚úÖ {query_count} {source_info['query_term'].lower()} provided")
            
            with col3:
                if schema_ddl and config.get('enable_ai_analysis', False):
                    st.markdown('<span class="feature-badge badge-ai">ü§ñ Ready for AI Analysis</span>', unsafe_allow_html=True)
    
    with tab4:
        render_compatibility_analysis_tab(config, schema_ddl if 'schema_ddl' in locals() else "", 
                                         queries_text if 'queries_text' in locals() else "")
    
    with tab5:  # Auto-fix tab
        render_enhanced_autofix_tab(config, schema_ddl if 'schema_ddl' in locals() else "", 
                                   queries_text if 'queries_text' in locals() else "")
    
    with tab6:
        render_enhanced_cost_analysis_tab(config)
    
    with tab7:
        render_enhanced_security_tab(config, schema_ddl if 'schema_ddl' in locals() else "")
    
    with tab8:
        render_aws_mapping_tab(config)
    
    with tab9:
        render_migration_scripts_tab(config, schema_ddl if 'schema_ddl' in locals() else "")
    
    with tab10:
        # Prepare migration context for AI analysis
        migration_context = {
            'source_engine': config['source_engine'],
            'target_engine': config['target_engine'],
            'schema_ddl': schema_ddl if 'schema_ddl' in locals() else "",
            'queries_text': queries_text if 'queries_text' in locals() else "",
            'data_size_gb': config.get('storage_gb', 100),
            'business_context': f"Migration from {config['source_engine']} to {config['target_engine']}",
            'business_critical': config.get('business_critical', False),
            'downtime_tolerance': config.get('downtime_tolerance', 'Unknown'),
            'team_experience': 'intermediate',  # Default
            'budget_constraints': 'moderate'    # Default
        }
        
        render_enhanced_ai_analysis_tab(config, migration_context)
    
    # Enhanced professional footer
    st.markdown("""
    <div style="margin-top: 3rem; padding: 2rem; background: linear-gradient(135deg, #f8fafc 0%, #f1f5f9 100%); border-radius: 12px; text-align: center; border: 1px solid #e5e7eb;">
        <h4>üóÑÔ∏è Enterprise Database Migration Analyzer</h4>
        <p><strong>Advanced AI-Powered Migration Intelligence ‚Ä¢ Real-time Cost Optimization ‚Ä¢ Enterprise Security & Compliance</strong></p>
        <div style="margin-top: 1rem;">
            <span class="feature-badge badge-new">üí∞ Live AWS Pricing</span>
            <span class="feature-badge badge-ai">ü§ñ AI-Enhanced</span>
            <span class="feature-badge badge-enterprise">üîí Enterprise Security</span>
            <span class="feature-badge badge-enhanced">üë• Team Collaboration</span>
        </div>
        <p style="font-size: 0.9rem; margin-top: 1rem; opacity: 0.9;">
            üî¨ Schema Analysis ‚Ä¢ üîç Query Compatibility ‚Ä¢ üìú Script Generation ‚Ä¢ ‚òÅÔ∏è AWS Integration ‚Ä¢ ü§ñ AI Recommendations ‚Ä¢ üìö Migration Examples ‚Ä¢ üí∞ Cost Optimization ‚Ä¢ üîí Security Assessment
        </p>
    </div>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()